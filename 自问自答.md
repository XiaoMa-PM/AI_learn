
## LLM系列
---

请用你自己的话（或者我们之前用过的比喻）简要解释**自注意力机制 (Self-Attention)** 的工作原理。它为什么叫“自”注意力？

我的：
自注意力机制，区别传统的词向量表达形式，他是通过生成所有词的QKV，让每一个词的Q去查询上下文的K，进行权重计算，输出score、最后与V积而得出新的向量矩阵。

参考：
- 自注意力机制的核心是**计算句子中每个词与其他所有词的关联度（或称注意力分数）**。
- 它通过Q, K, V三个向量来实现：每个词的Q（查询）会去和所有词的K（键）进行匹配，得到一个分数，这个分数决定了应该从对应词的V（值）中提取多少信息。
- 之所以叫“**自**”注意力，是因为这个注意力的计算完全发生在一个句子**内部**，是句子中的词**自己和自己**进行关联度计算，没有外部信息的介入。
---

同样是预训练语言模型，为什么说 **GPT** 天生更适合**生成任务**（如写一首诗），而 **BERT** 天生更适合**理解任务**（如情感分类）？请从它们的预训练方式或模型结构来解释。

我的：
GPT采用了Only-Decoder架构，其中的机制是生成一个Token后去识别上文，进一步生成下一个Token。
BERT采用了Only-Encoder架构，核心在于Mask-self attention，进行随机内部掩码的模式训练，导致其机制更加符合做理解词义任务。

参考：
- **GPT**之所以擅长**生成**，是因为它的**预训练任务**是**预测下一个词**。这种从左到右的“文字接龙”式训练，让它的模型结构和训练目标天然就是为了“创造”和“延续”文本。它的**单向注意力**机制（只能看到前面的词）也完全符合生成任务的特点。
- **BERT**之所以擅长**理解**，是因为它的**预训练任务**是**完形填空**。为了猜出被遮盖的词，它必须深刻理解完整的、双向的上下文语境。它的**双向注意力**机制让它在处理句子时能拥有全局视角，因此对整个句子的语义把握得更准，适合做分类、判断等理解性任务。