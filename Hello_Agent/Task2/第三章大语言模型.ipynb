{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b35a7e3",
   "metadata": {},
   "source": [
    "# 第三章 大语言模型基础\n",
    "## 语言模型与Transformer架构\n",
    "N-gram到RNN\n",
    "核心：LM是自然语言处理的核心\n",
    "目的：计算词序概率\n",
    "\n",
    "**N-gram模型：**（基于马尔可夫假设，第n个词与前面n+1个词有关）\n",
    "    基于统计的模型，假设当前词只依赖于前n-1个词\n",
    "    公式：P(w_i|w_1,w_2,...,w_i-1) = P(w_i|w_i-n+1,w_i-n+2,...,w_i-1)\n",
    "痛点：\n",
    "    - 数据稀疏：未出现的词组出现则概率为0；\n",
    "    - 泛化能力差：无法理解词语的“含义”，无法实现词义关联。\n",
    "N-gram模型的词变成了孤立、离散的符号。\n",
    "\n",
    "**神经网络语言模型（NNLM）：**\n",
    "神经网络思路\n",
    "    目的是解决泛化能力不足，是用了词嵌入（word embedding）技术。\n",
    "    词嵌入：将词语映射到低维连续向量空间，捕捉词语的语义信息。\n",
    "    如：`king`-`man`+`woman`= `queen`\n",
    "痛点：上下文窗口**固定大小**，仅处理固定前文数量信息。\n",
    "\n",
    "**RNN/LSTM：**\n",
    "RNN打破**固定大小**限制\n",
    "LSTM通过门控机（门控单元）解决了RNN的梯度消失问题。“长期依赖问题”\n",
    "痛点：\n",
    "    - 串行计算：每个时间步的计算依赖于前一个时间步的隐藏状态，无法并行计算。\n",
    "\n",
    "**Transformer架构：**\n",
    "目的：捕获长距离依赖，又实现并行计算的模型\n",
    "源于2017年的论文《Attention Is All You Need》\n",
    "抛弃“循环结构”，完全依赖“注意力机制”实现。\n",
    "\n",
    "> Thinking：使用注意力机制实现，具有什么局限性？相比于循环结构？是否和两者优雅结合？\n",
    "\n",
    "## LLM核心架构\n",
    "**Transformer架构：**\n",
    "Encoder：负责理解输入序列的上下文信息。（英文的句子输入）【完形填空】\n",
    "Decoder：根据Encoder的输出，生成目标序列。（中文的句子输出）【词语接龙】\n",
    "\n",
    "**Decoder-Only架构：**\n",
    "只做**预测下一个词**的工作，不依赖Encoder的输出。\n",
    "工作模式在于**自回归**（ autoregressive），即每次只预测一个词，基于已预测的词序列。\n",
    "优点：\n",
    "- 训练目的单一，制作一个工作\n",
    "- 结构简单，易规模化扩张\n",
    "- 天然适合生成任务\n",
    "\n",
    "> 用Encoder+Decoder就训练的大语言模型相比于Decoder-Only架构，有什么优势？为什么原有架构会被摒弃？\n",
    "\n",
    "> 为什么输出token会比输入token贵，在于使用自回归长度会增加模型的计算量。那如果用Encoder+Decoder架构，计算量会是怎么样的？\n",
    "\n",
    "**相关概念：**\n",
    "- Self-Attention：Query、Key、Value\n",
    "    - Query：当前词的向量表示\n",
    "    - Key：所有词的向量表示\n",
    "    - Value：所有词的向量表示\n",
    "- Multi-Head Attention：\n",
    "    - 并行计算多个注意力头，每个头关注不同的信息。\n",
    "    - 每个头的输出拼接起来，作为最终的输出。\n",
    "- Positional Encoding：\n",
    "    - 解决自注意力机制无序问题\n",
    "    - 通过`sin/cos`函数，为每个位置添加一个唯一的向量表示。(绝对位置)\n",
    "- Masked Self-Attention：\n",
    "    - 解决并行计算问题\n",
    "    - 用于Decoder层，防止模型“偷看”未来的信息。\n",
    "    - 在计算注意力时，将未来的位置的Key和Value设置为0。\n",
    "- 残差连接（Residual Connection）：\n",
    "    - 每个子层（如Self-Attention或Feed-Forward Network）的输出，与输入相加。\n",
    "    - 解决了梯度消失问题，使得模型更容易训练。\n",
    "- 层归一化（Layer Normalization）：\n",
    "    - 对每个样本的所有特征进行归一化，解决了内部协变量_shift_问题。\n",
    "    - 每个样本的输出，都有相同的分布，使得模型更容易训练。\n",
    "- 前馈神经网络（Feed-Forward Network）：\n",
    "    - 每个位置的向量，通过全连接层，映射到一个新的向量表示。\n",
    "    - 用于对序列中的每个位置进行特征提取和变换。\n",
    "\n",
    "## LLM交互与使用的Point\n",
    "**提示词工程（Prompt Engineering）：**\n",
    "- 技术：Zero-shot、One-shot、Few-shot、思维连CoT\n",
    "- 关键演进：指令调优\n",
    "- 采样参数：\n",
    "    - 温度（Temperature）：控制模型输出的随机性。\n",
    "    - Top-k Sampling：只从概率最高的k个词中采样。\n",
    "    - Nucleus Sampling（Top-p Sampling）：从概率最高的词中，累计概率超过p的词中采样。\n",
    "\n",
    "**文本分词（Tokenization）：**\n",
    "- 目的：将文本转换为模型可以处理的离散 tokens。\n",
    "- 方法：\n",
    "    - 基于字符（Character-level Tokenization）\n",
    "    - 基于子词（Subword-level Tokenization）\n",
    "    - 基于单词（Word-level Tokenization）\n",
    "- 影响：\n",
    "    - 不同的分词方法，会影响模型的性能和效率。\n",
    "    - 如：基于子词的方法，能够处理未出现过的词，而基于单词的方法，对拼写错误更敏感。\n",
    "\n",
    "**模型部署与选型：**\n",
    "- 部署略\n",
    "- 选型：\n",
    "    - 性能\n",
    "    - 成本\n",
    "    - 速度\n",
    "    - 上下文窗口\n",
    "    - 部署方式\n",
    "    - 生态\n",
    "    - 可微调性\n",
    "    - 安全性\n",
    "\n",
    "## LLM所面临的挑战\n",
    "**缩放法则：**\n",
    "\n",
    "**模型幻觉：**\n",
    "\n",
    "**知识时效性：**\n",
    "\n",
    "**偏见：**\n",
    "\n",
    "**Agent的解决之道：**\n",
    "\n",
    "## 课程建设建议\n",
    "Transformer部门过于强调代码，建议引入`Happy-LLM`课程的代码详解，此处作其原理粗略解读，以及每个节点的目的/原理/实现方式/局限性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd009f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
