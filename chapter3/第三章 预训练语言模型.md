# 第三章 预训练语言模型
---

## 一、Encoder-only PLM
---
Google：Encoder-Only->Natural Language Model（NLM）->BERT
Google：Encoder+Decoder->T5模型
OpenAi：Decoder-Only->Natural Language Generation（NLG）->GPT

### 1. BERT
来源：放弃传统的RNN、LSTM思路的Transformer模型，使用了ELMo预训练思路。

作用：对输入文本进行深度、双向的理解，并产出高质量的上下文词向量。

**如何训练**：
**阶段一**：完型填空（Masked Language Model，MLM）
- “双向”能力来源
- 工作机制：掩盖一个句子中某一个词，让BERT去预测出mask的词是什么
- 实现：**理解句子内部关系**

**阶段二**：句子关系判断（Next Sentence Prediction，NSP）
- 工作机制：
	- 语料库提取`A&B`句子
	- `B`有可能是`A`的下一句（标签`IsNext`），也有可能是随机提取无关句子（标签`NotNext`)
	- 句子特殊处理，让BERT判断句子连续性
- 实现：**句子与句子之间的逻辑关系**
- 有利于做问答QA、自然语言推理NLI句子关系工作

**应用**：
BERT作为专精所有的**自然语言理解（Natural Language Understanding，NLU）** 相关的任务
使用如下：
- 大型语料库->预训练Encoder-Only->BERT模型
- 针对具体任务，在BERT顶层设置一个**小分类器tokenizer**（e.g. 全连接层）
- 用自己任务的标注数据对这个“BERT+小分类器”整体模型进行**微调（Fine-tuning）**，用较小学习率进行再训练
作用：降低了NLP任务门槛与成本，不需要从0->1训练大模型

**BERT模型结构：**
![](inbox/Pasted%20image%2020250902101202.png)

下面一步步拆开讲解：
![](inbox/Pasted%20image%2020250902101335.png)
**起点：Tokenizer部分**
1. 接受文本Text
2. 分词与ID化Tokenizer->input_ids
3. ID化：每个词转换为唯一的数字ID

**核心流程：BERT Model
1. Embedding
	- 模型接受input_ids，将数字ID转化为初始的，包含基础语义的向量
	- Embedding：**词嵌入（Token Embedding）、位置嵌入（Positional Embedding）、句子类型嵌入（Segment Embedding）** 这三个向量相加，形成每个词元最原始、包含所有必备信息的输入向量->hidden_states。


**总结BERT**：
“深度的双向语言理解器”->堆叠Transformer Encoder0>实现MLM与NSP预训->实现上下文深刻理解。
建立了“预训练+微调”范式，在[4. ELMo](../chapter1/第一章%20NLP基础概念.md#4.%20ELMo)的基础上更进一步。

BERT是预训练的开始，之后延伸出了MacBERT、RoBERTa、ALBERT、DeBERTa。

### 2. RoBERTa
Robustly Optimized BERT Approach

### 3. ALBERT
A Lite BERT

## 二、Encoder-Decoder PLM
---

### 1. T5


## 三、 Decoder-Only PLM
---

### 1.GPT

### 2. LLaMA

### 3. GLM

