# 第三章 预训练语言模型
---

## 一、Encoder-only PLM
---
Google：Encoder-Only->Natural Language Model（NLM）->BERT
Google：Encoder+Decoder->T5模型
OpenAi：Decoder-Only->Natural Language Generation（NLG）->GPT

### 1. BERT
来源：放弃传统的RNN、LSTM思路的Transformer模型，使用了ELMo预训练思路。

作用：对输入文本进行深度、双向的理解，并产出高质量的上下文词向量。拆解传统：训练+微调，让预训练于微调各司其职。

**如何训练**：
**阶段一**：完型填空（Masked Language Model，**MLM**）
- “双向”能力来源
- 工作机制：掩盖一个句子中某一个词，让BERT去预测出mask的词是什么
- 实现：**理解句子内部关系**

**阶段二**：句子关系判断（Next Sentence Prediction，**NSP**）
- 工作机制：
	- 语料库提取`A&B`句子
	- `B`有可能是`A`的下一句（标签`IsNext`），也有可能是随机提取无关句子（标签`NotNext`)
	- 句子特殊处理，让BERT判断句子连续性
- 实现：**句子与句子之间的逻辑关系**
- 有利于做问答QA、自然语言推理NLI句子关系工作

**应用**：
BERT作为专精所有的**自然语言理解（Natural Language Understanding，NLU）** 相关的任务
使用如下：
- 大型语料库->预训练Encoder-Only->BERT模型
- 针对具体任务，在BERT顶层设置一个**小分类器tokenizer**（e.g. 全连接层）
- 用自己任务的标注数据对这个“BERT+小分类器”整体模型进行**微调（Fine-tuning）**，用较小学习率进行再训练
作用：降低了NLP任务门槛与成本，不需要从0->1训练大模型

**BERT模型结构：**
![](inbox/Pasted%20image%2020250902101202.png)

下面一步步拆开讲解：
![](inbox/Pasted%20image%2020250902101335.png)
**起点：Tokenizer部分**
1. 接受文本Text
2. 分词与ID化Tokenizer->`input_ids`
3. ID化：每个词转换为唯一的数字ID

**核心流程：BERT Model
1. Embedding
	- 模型接受`input_ids`，将数字ID转化为初始的，包含基础语义的向量
	- Embedding：**词嵌入（Token Embedding）、位置嵌入（Positional Embedding）、句子类型嵌入（Segment Embedding）** 这三个向量相加，形成每个词元最原始、包含所有必备信息的输入向量->`hidden_states`。
2. Encoder
	- 初始化后的`hidden_states`送入编码器，Encoder Block 由多个**Encoder Layer**组成
	- 向量流入后逐层向上处理，每一层会让每个词向量表示融合更多、更丰富的上下文信息
3. Prediction Heads
	- 向量数据->EncoderLayer->`hidden_states`最后，每个词的向量都蕴含了对整个句子的深刻理解
	- `Prediction_heads`属于预测头，这是一个小型的“完形填空”**MLM**网络，内部为`linear->激活函数->Linear->output`的过程就是一个`Prediction_heads`预测头结构。
![](inbox/Pasted%20image%2020250902215431.png)

拆解内部：**EncoderLayer**：
1. Attention
	- `hidden_states`流入Attention模块，所有词的向量互相交换信息，输出的`hidden_states`是**全局信息**交换
2. Intermediate/FNN
	- 前馈神经网络，每个词汇**单独**进行**非线性和特征**提取
3. “+”残差连接-Add&Norm
	- 图中`residual`进行了Attention&FNN前的复制回传。保证了梯度顺畅回传。

![](inbox/Pasted%20image%2020250902215706.png)
拆解内部：**Attention**：
1. 生成Q，K，V：`hidden_states`进行线性变换，分出`query_states` (Q), `key_states` (K), `value_states` (V)
2. 计算注意力分数：`query` 和 `key` 进行 `QK^T` (点积)=Score（此处还有一步`sqrt(d_K)`)
3. 分数归一化：`soft_max`函数，将Score调整到\[0，1\]、总和为1的注意力权重->`attention_weight`。此处形成了：**每个词分配多少“注意力”**
4. 加权求和：`attention_weight`于`valur_states`进行矩阵相乘。最终输出`attention_out`。

![](inbox/Pasted%20image%2020250902220707.png)


**总结BERT**：
“深度的双向语言理解器”->堆叠Transformer Encoder0>实现MLM与NSP预训->实现上下文深刻理解。
建立了“预训练+微调”范式，在[4. ELMo](../chapter1/第一章%20NLP基础概念.md#4.%20ELMo)的基础上更进一步。

BERT是预训练的开始，之后延伸出了MacBERT、RoBERTa、ALBERT、DeBERTa。

### 2. RoBERTa
Robustly Optimized BERT Approach--Facebook AI（**扩大训练参数**）
鲁棒优化的BERT方法

RoBERTa系统性研究了BERT的训练过程，找到提升性能的方法。新的一个模型结构，实现了：**训练方法和细节，与模型架构本身同样重要，甚至更重要。**

**核心优化4个优化点**：
**优化一**： 训练数据与时长
- BERT：使用了16GB文本数据（英文维基百科+图书预料库）进行训练
- RoBERTa：使用了160GB的高质量文本数据，在BERT上增加了预料（大量新闻、网页预料）。数据量是BERT的10倍。同时，训练步数和批大小（Batch Size）也远超BERT。

**优化二**：取消NSP任务（制作好一个核心）
- BERT：MLM+NSP两个任务
- EoBERTa：NSP任务过于冗余（原因：只能学习句子之间的表面特征，而不是真正的逻辑推理能力）
- 做法：去掉NSP任务，只保留了MLM，不去刻意划分句子，而是直接1个/n个文档连续抽取512个词元进行文本块训练。
- 作用：不去区分A/B句子是否相连，只专注于MLM，实现了上下文更加深刻的理解。

**优化三**：动态掩码Dynamic Masking（训练方式多样化）
- BERT：采用静态掩码Static Masking。数据预处理阶段，一句话中哪些词被`[Mask]`掩盖，都是一次性确定不再变化。这导致训练过程，模型看到同一句话，永远都是同一个词被掩盖。
- RoBERTa：采用动态掩码Dynamic Masking。预先生成`[Mask]`的数据集，而是每次向模型输入一个序列，实时、随机地生成一个新的掩盖模型。
- 作用：每次“完形填空”都不一样，**增强了语言泛化能力**。

**优化四**：Byte-Level BPE（更大的词汇表）
- BERT：基于**字符**的Byte-Pair Encoding词汇表，大小约3万。
- RoBERTa：基于**字节**的BPE，词汇表大小约5万，能够处理未登录词问题。

BPE是Tokenizer的一种处理方式：
- 基于字符：
	- 流程：一句话->拆分为基础字符->按照BPE算法，讲`t`和`h`此类合并为`th`的字符块，最后达到预设的字符数量，e.g. 3万。
	- 缺点：原有字符预料库内不存在的字符，emoji😂，导致其无法识别，被贴上`[UNK]`，信息丢失
- 基于字节：
	- 基础：计算机记录字符，都是使用256基础字节序列`000001111`此类
	- 流程：所有文本->转回基础字节序列->在字节序列上，去和BPE算法结合，把`t`和`h`的字节序列合并。
	- 核心：语料库解决了不存在字符无法识别的问题，都是计算机的字节序列。实现万物可以分词。
优秀的模型架构基础上：**更大规模数据、更长时间训练、更优化的训练策略**

| 对比维度        | 基于字符的BPE (BERT)                                      | 基于字节的BPE (RoBERTa/GPT)                                      |
|-----------------|----------------------------------------------------------|------------------------------------------------------------------|
| 基础单位        | 语料库中的所有字符                                       | 全宇宙统一的256个字节                                            |
| 未知词 (OOV)    | 遇到不在初始字符集里的字符时，会产生 [UNK] ，丢失信息。   | 永不产生 [UNK] ，能编码任何字符串，不丢失信息。                   |
| 鲁棒性          | 对多语言、表情符号、错别字、噪音文本的处理能力较差。       | 极强，真正做到了语言无关 (language-agnostic)，对各种文本形式都非常鲁棒。 |
| 词汇表大小      | 初始字符集可能很大（比如中日韩文），最终词汇表也较大。     | 基础词汇表极小 (256)，可以从一个非常小的基础构建出一个同样大小但表达能力更强的词汇表。 |
### 3. ALBERT
A Lite BERT（**减小训练参数**）

思想：减少参数量、保持模型性能：
- 词嵌入层+大规模参数->低维度的词嵌入+投影层
- N层的Self-Attention+FNN
作用：降低训练成本、内存占用、增快推理速度。

**核心两个优化点+一个新Task：**
**优化点一**：词嵌入矩阵分解 (Factorized Embedding Parameterization)
- BERT：词嵌入矩阵的大小为`V*H`,其中`V`上词汇表大小（比如3000），`H`为模型隐藏层大小（比如768）。这个矩阵参数量非常大（3000 * 768）。BERT的处理是把“单词的独立语义”和“准备送入Transformer的上下文语义”耦合在一个大矩阵进行训练学习。
- ALBERT：将巨大的矩阵分解为两个小矩阵
	- 先学习低维度的词嵌入矩阵`V*E`，其中`E`远小于`H`（比如128）。该矩阵负责学习每个单词的、与上下文无关的“基础含义”。
	- 通过投影层`E*H`将E放大到Transformer需要的H维上
- 作用：词嵌入层参数量降低80%

**优化点二**：跨层参数共享 (Cross-layer Parameter Sharing)
- BERT：N层的EncoderLayer，每一层都有自己独立的一套参数
- ALBERT：EncoderLayer共享同一套参数。模型只学习一组Attention参数和一组FNN参数，然后处理文本时，将这组参数重复使用12次。
- 作用：Encoder编码器参数量爆减1/N。

**新的工作：** 句子顺序预测(Sentence-Order Prediction, SOP)
- NSP问题：负样本（NotNext）是从其他文档随机抽取，与当前句子完全不相关，不利于模型学习连贯性，且冗余
- SOP做法：
	- 正样本：从语料库中选取连续的两个句子，顺序不变
	- 负样本：将这两个连续句子，**顺序颠倒**
- 作用：负样本与正样本来自同个文档，分类上一致。强迫模型学习更深层次的**语篇连贯性 (Discourse Coherence)**。

权衡：
- 参数效率高
- 训练和推理速度慢：时间换空间（参数流过12层）

### 总结


## 二、Encoder-Decoder PLM
---

### 1. T5


## 三、 Decoder-Only PLM
---

### 1.GPT

### 2. LLaMA

### 3. GLM

