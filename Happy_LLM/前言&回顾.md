![](inbox/Pasted%20image%2020250821094114.png)
# 一、NLP 发展脉络与 LLM 的突破性
---

## 1. NLP技术演进
---

从符号主义、统计学习到深度学习阶段，最终进入 ​**预训练语言模型（PLM）​**​ 时代（如 BERT、GPT）。PLM 基于 Transformer 架构，通过预训练-微调范式提升语言理解能力，但仍依赖监督数据

## 2. LLM质变
---

LLM 在 PLM 基础上通过 ​**千亿级参数规模**、**海量无监督数据训练**​ 及 ​**指令微调/人类反馈强化学习（RLHF）​**​ 实现突破，具备三大核心能力：
- **涌现能力**​：质变准确率提升；
- **上下文学习（In-context Learning）​**​：通过示例泛化任务；
- **指令理解与流畅生成**​：逼近通用人工智能（AGI）目标。


# 二、 书籍结构与内容
---

## 基础知识
---

目录结构：
- [第一章 NLP基础概念](chapter1/第一章%20NLP基础概念.md)
- [第二章 Transformer架构](chapter2/第二章%20Transformer架构.md)
- [第三章 预训练语言模型](chapter3/第三章%20预训练语言模型.md)
- [第四章 大语言模型](chapter4/第四章%20大语言模型.md)
- [第五章 动手搭建大模型](chapter5/第五章%20动手搭建大模型.md)
- [第六章 大模型训练流程实践](chapter6/第六章%20大模型训练流程实践.md)
- [第七章 大模型应用](chapter7/第七章%20大模型应用.md)


## 任务
---

![](inbox/42c3ed95ee69120a3fa9b6a67e8553e9.jpg)

# 三、一期Learn回顾
---
## [第一章 NLP基础概念](chapter1/第一章%20NLP基础概念.md)
---

### 1. 学习NLP的应用
**NLU（自然语言理解-Natural Language Understanding）**：让计算机**理解**自然语言、**输入端工作**

**NLG (自然语言生成 - Natural Language Generation)**：让计算机**生成**自然语言、**输出端工作**

| 应用场景         | 核心        | 简单解释（大白话）                                           |
| ------------ | --------- | --------------------------------------------------- |
| 情感分析         | NLU       | 判断一段话是开心的、伤心的还是中性的。（比如分析商品评论是好评还是差评）                |
| 文本分类         | NLU       | 给一篇文章打上标签，比如“体育”、“娱乐”、“科技”。（比如新闻APP的频道分类）（比如分类垃圾邮件） |
| 命名实体识别 (NER) | NLU       | 从一句话里找出人名、地名、公司名等特定信息。（比如从简历中自动提取姓名和电话）             |
| 机器翻译         | NLU + NLG | 先“理解”源语言（NLU），再“生成”目标语言（NLG）。                       |
| 文本摘要         | NLU + NLG | 先“读懂”整篇文章的核心意思（NLU），再“写出”一段简短的摘要（NLG）。              |
| 问答系统/对话机器人   | NLU + NLG | 先“理解”你的问题（NLU），然后组织语言“生成”答案（NLG）。                   |

### 2. 学习文本表示的发展（Text Representation）
**Level 1: One-hot 编码 (离散表示)**
**方法：** 想象一下，我们有一本包含10000个不同词的词典。为了表示“苹果”这个词，我们就创建一个有10000个位置的列表（向量），在代表“苹果”的那个位置上写个`1`，其他所有位置都写`0`。
**局限性：**
1. **维度灾难：** 如果词典有10万个词，那每个词都要用一个10万维的列表表示，太占空间了。
2. **语义鸿沟：** 它无法表达词与词之间的关系。在One-hot的世界里，"国王" 和 "女王" 的距离，跟 "国王" 和 "香蕉" 的距离是一样远的，这显然不合理。

**Level 2: Word Embedding (词嵌入/分布式表示)**
为了解决One-hot的缺点，大神们想出了`Word Embedding`，其中最经典的代表就是 **Word2Vec**。
**方法：** 不再用一个孤立的 `1` 来表示词，而是用一个相对低维（比如300维）且每个位置都是小数的向量来表示。这个向量是通过一个神经网络，根据大量文本（比如维基百科）学习出来的。
**核心思想：** **“物以类聚，词以群分”**。一个词的意思，由它上下文中的词来决定。所以，经常出现在相似语境中的词，它们的向量在空间中的位置就会很接近。
**改进：**
1. **解决了维度问题**：从几万维降到了几百维。
2. **包含了语义信息**： "国王" 和 "女王" 的词向量会非常相似。甚至可以进行有趣的数学计算，比如 `vector('国王') - vector('男人') + vector('女人')` 的结果会非常接近 `vector('女王')`。

## [第二章 Transformer架构](chapter2/第二章%20Transformer架构.md)
---
### 1. Why Is Transformer？
在Transformer出现之前，处理序列文本（比如一句话）的主流模型是 **RNN (循环神经网络)** 和它的变种 **LSTM**。它们就像一个一个地读单词的“老学究”，按顺序处理信息。

这种方式有两个主要问题：
1. **无法并行计算，速度慢**：必须处理完第一个词，才能处理第二个词，就像多米诺骨牌，无法同时进行。当句子很长时，效率极低。
2. **长距离依赖问题**：当一句话很长时，模型很容易“忘记”前面说的内容，导致理解能力下降。
**Transformer的诞生，就是为了从根本上解决这两个问题。**

### 2. What Is Transformer？
目标是：**深刻理解句子中每个词的含义，以及它们之间错综复杂的关系**

**准备阶段：给每个词“建档案” (对应：词嵌入 + 位置编码)**
团队接到一句话，比如：“我昨天在公园里看见一只可爱的猫”。
- **第一步：基础档案 (Word Embedding)** 团队里的每个“词”（比如“我”、“公园”、“猫”）都不是一个孤立的文字，而是自带丰富信息的“成员”。这些信息就是你在第一章学到的**词嵌入 (Word Embedding)**，一个包含语义的向量。
- **第二步：位置档案 (Positional Encoding)** Transformer为了追求速度，是**同时看到**所有词的，它不像RNN那样有先后顺序。但语序很重要（“猫咬狗”和“狗咬猫”完全不同）。怎么办？ **解决方法：** 强制给每个词的“基础档案”上，额外添加一个包含位置信息的“工位号”或“坐标”。这就是**位置编码 (Positional Encoding)**。
所以，团队拿到的最终材料是：

词的最终表示=词嵌入向量+位置编码向量

这样，每个词既知道了自己是谁，也知道了自己在哪里。

**核心工作：开“圆桌会议”讨论 (对应：自注意力机制 Self-Attention)**
- **要解决的问题：** 理解“可爱的”是修饰“猫”的，“我”是“看见”的主体。词与词之间存在着密切的关联。
- **解决方法 (Self-Attention)：** 让句子中的**每一个词**，都去和其他所有词（包括自己）“互动”一次，计算一个“**关联度分数**”。这个分数越高，说明这两个词的关系越紧密，越应该多“关注”对方。

为了完成这个计算，每个词都被赋予了三个身份（三个不同的向量），这也是你笔记里提到的**Q, K, V**：
- **Query (查询向量 Q):** 代表“我”这个词，主动去寻找和自己相关的词。可以理解为“我要找谁？”
- **Key (键向量 K):** 代表其他词，用来被Q查询和匹配。可以理解为“我是不是你要找的？”
- **Value (值向量 V):** 代表其他词自身的实际信息。可以理解为“这是我的信息，拿去用吧。”

**计算过程：** “我”的Q，去和所有词的K（包括“我”自己的K）计算相似度，得到一个“关注度分数”。然后用这个分数去加权求和所有词的V，得到一个全新的、融合了全局上下文信息的“我”的向量。
**一句话总结自注意力：** 它让每个词都能“看到”句子中的所有其他词，并动态地计算出谁对理解自己最重要，从而生成一个包含丰富上下文信息的新表示。

**升级工作：开“分组讨论会” (对应：多头注意力机制 Multi-Head Attention)**
- **解决方法 (Multi-Head Attention)：** 一次会议不够，那就同时开好几个！把原始的Q, K, V“复制”并转换成多组（比如8组），每组都独立进行一次自注意力计算。 
- 这就像团队里分成了8个**专家小组**，有的专门分析语法，有的专门分析语义... 每个小组都从不同角度去分析词与词的关系，最后把所有小组的结论**拼接**起来，得到一个更全面、更丰富的理解。

**辅助工作：“独立思考”与“信息整合”**
- **前馈神经网络 (Feed-Forward Network):** 在开完激烈的“分组讨论会”（多头注意力）后，每个词得到的信息已经非常丰富了。这时需要让每个词的“代表”回到自己的办公室**独立思考、消化吸收**一下。这个过程就是通过一个简单的全连接神经网络（FFN），对信息进行一次非线性变换，增强模型的表达能力。
- **残差连接 (Add) & 层归一化 (Norm):** 这两个是你笔记里提到的“Add & Norm”，它们是帮助这个“团队”更好工作的“辅助工具”，主要作用是让模型训练更稳定、更高效。
	- **Add (残差连接):** 像是在每次处理后，都把最原始的信息再加回来一次，确保在信息传递的多层网络中，原始的重要信息不会丢失。公式是 `$X_{new} = X_{old} + \text{SubLayer}(X_{old})$`。
	- **Norm (层归一化):** 像是每次会议后统一一下大家的“汇报标准”，防止某些成员的“嗓门”过大或过小，影响最终决策，让数据分布更稳定。

### 3. How Do Transformer Work？
**整体架构：分工明确的两个部门 (Encoder & Decoder)**
- **编码器 (Encoder / 左半部分):** 就是我们上面描述的“阅读理解团队”。它由 N 个相同的“注意力层+前馈网络层”堆叠而成。它的唯一职责就是：**接收输入的句子，并对它进行深度、充分的理解**，最终输出一系列富含上下文信息的向量。
- **解码器 (Decoder / 右半部分):** 像是一个“写作专家”。它的职责是**根据编码器理解好的信息，生成目标句子**。它比编码器多了一个注意力层，这个层会去“偷看”编码器的输出，确保生成的内容与输入相关。（这在机器翻译等任务中至关重要）。
![](inbox/Pasted%20image%2020250830015353.png)

**发展现状**：
现在主流的很多大模型，其实是基于纯Encoder架构（如BERT）或纯Decoder架构（如GPT系列）进行改造和扩展的。例如常听到的 **GPT**，就是 "Generative Pre-trained **Transformer**"，它主要使用了Transformer的Decoder部分，并将其堆叠得非常深，专注于“生成”任务。

**Transformer的成功 = 位置编码 (解决语序) + 自注意力 (核心，并行理解上下文) + 多头注意力 (多角度理解) + 辅助工具 (Add & Norm, FFN) + 宏观架构 (Encoder-Decoder)**

它通过**完全抛弃RNN的序列依赖**，实现了**高效的并行计算**，并通过**自注意力机制**完美地解决了长距离依赖问题。可以说，**没有Transformer，就没有今天的ChatGPT等大语言模型**。

## [第三章 预训练语言模型](chapter3/第三章%20预训练语言模型.md)
---
### 核心思路：预训练-微调(Pre-train & Fine-tune)
- **没有预训练的时代**：每当有一个任务（比如情感分析），我们就招一个“零基础”的新学生（新模型），然后只给他情感分析的教材让他学。他的知识面会非常窄，效果也有限。
- **有预训练的时代**：我们先找一个学生，让他**阅读互联网上几乎所有的书籍和文章（通识教育）**，让他对语言、语法、事实、逻辑有一个全面而深刻的理解。这个过程，就叫做 **“预训练” (Pre-training)**。
	- 然后，当有一个具体任务时（比如做情感分析），利用**预训练模型**，给他一点点情感分析的专业教材，让他稍微“点拨”一下（这个过程叫 **“微调” Fine-tuning**），他就能做得非常好。

### 预训练模型发展
**起点：Word2Vec 的局限**
Word2Vec的局限性是“静态的”。
无论“苹果”出现在什么句子里，它的向量都是一样的。这就像学生只知道“苹果”这一个词的孤立意思，不懂它在不同语境下的含义。

**第1站：ELMo - 让词向量“动”起来**
- **是什么**：ELMo 是第一个伟大的尝试，它解决了“一词多义”的问题。它利用了LSTM（Transformer之前的主流模型），根据一个词的**上下文**，来动态地生成这个词的词向量。
- **好在哪**：“我爱吃苹果”和“我想买苹果手机”中的“苹果”，在ELMo那里会得到**两个不同**的向量。这是一个巨大的进步。
- **不足**：它用的还是老旧的、无法并行计算的 LSTM 架构，而且它的特征提取能力也不如后来者强大。

**第2站：GPT & BERT - Transformer时代的“绝代双骄”**
Transformer在CV的出现，让NLP处理开始运用Transformer 架构来做预训练。

其中有两种经典应用方式如下：
1. **GPT (Generative Pre-trained Transformer)**
- **核心组合**：GPT = Transformer的解码器 (Decoder) + 大规模预训练
- **学习方法 (预训练任务)**：**自回归 (Autoregressive)**。简单说，就是“**预测下一个词**”。
	- 给它一句话的前半部分，比如 `“今天天气真不错，我们一起去”`，让它预测下一个最可能的词是 `“公园”`。它就在海量的文本上日复一日地做这种“接话”练习。
- **特点**：因为它的学习方式是从左到右预测下一个词，所以它天生就是**单向**的。这使得它极其擅长**生成式任务**，比如写文章、写代码、回答问题。它就是我们今天所用的大多数对话式AI（包括我）的直系祖先。

2. **BERT (Bidirectional Encoder Representations from Transformers)** 
- **核心组合**：BERT = Transformer的编码器 (Encoder) + 大规模预训练
- **学习方法 (预训练任务)**：**掩码语言模型 (Masked Language Model, MLM)**。简单说，就是“**做完形填空**”。
	- 把一句话中的某些词随机盖住（`[MASK]`），比如 `“今天天气真[MASK]，我们一起去公园”`，然后让BERT去猜测被盖住的词是什么。
- **特点**：为了猜对被盖住的词，BERT必须同时**观察左右两边的上下文**（“今天天气真...”和“...我们一起去公园”），所以它是**双向**的。这使得它对句子的**理解能力**极强，特别擅长**理解式任务**，如文本分类、情感分析、命名实体识别等。

### GPT vs.  BERT

| 特性          | GPT (以GPT-1为例)            | BERT                         |
| ----------- | ------------------------- | ---------------------------- |
| ​**核心架构**​  | Transformer Decoder       | Transformer Encoder          |
| ​**预训练任务**​ | 预测下一个词 (Autoregressive)   | 完形填空 (MLM)                   |
| ​**信息流向**​  | 单向 (只能看左边的信息)             | 双向 (能同时看左右两边的信息)             |
| ​**天然优势**​  | 生成 (Generation) 任务，如写作、对话 | 理解 (Understanding) 任务，如分类、判断 |
| ​**通俗比喻**​  | 成语接龙大师                    | 完形填空大师                       |

### 拓展：Transformer之外的架构
注意力机制；解决了无法并行计算和长距离依赖问题

卷积网络；解决了图像识别问题
随机森林；解决了过拟合问题
梯度下降；解决了模型训练问题