# 第三章 预训练语言模型
---

## 一、Encoder-only PLM
---
Google：Encoder-Only->Natural Language Model（NLM）->BERT
Google：Encoder+Decoder->T5模型
OpenAi：Decoder-Only->Natural Language Generation（NLG）->GPT

### 1. BERT
来源：放弃传统的RNN、LSTM思路的Transformer模型，使用了ELMo预训练思路。

作用：对输入文本进行深度、双向的理解，并产出高质量的上下文词向量。拆解传统：训练+微调，让预训练于微调各司其职。

**如何训练**：
**阶段一**：完型填空（Masked Language Model，**MLM**）
- “双向”能力来源
- 工作机制：掩盖一个句子中某一个词，让BERT去预测出mask的词是什么
- 实现：**理解句子内部关系**

**阶段二**：句子关系判断（Next Sentence Prediction，**NSP**）
- 工作机制：
	- 语料库提取`A&B`句子
	- `B`有可能是`A`的下一句（标签`IsNext`），也有可能是随机提取无关句子（标签`NotNext`)
	- 句子特殊处理，让BERT判断句子连续性
- 实现：**句子与句子之间的逻辑关系**
- 有利于做问答QA、自然语言推理NLI句子关系工作

**应用**：
BERT作为专精所有的**自然语言理解（Natural Language Understanding，NLU）** 相关的任务
使用如下：
- 大型语料库->预训练Encoder-Only->BERT模型
- 针对具体任务，在BERT顶层设置一个**小分类器tokenizer**（e.g. 全连接层）
- 用自己任务的标注数据对这个“BERT+小分类器”整体模型进行**微调（Fine-tuning）**，用较小学习率进行再训练
作用：降低了NLP任务门槛与成本，不需要从0->1训练大模型

**BERT模型结构：**
![](inbox/Pasted%20image%2020250902101202.png)

下面一步步拆开讲解：
![](inbox/Pasted%20image%2020250902101335.png)
**起点：Tokenizer部分**
1. 接受文本Text
2. 分词与ID化Tokenizer->`input_ids`
3. ID化：每个词转换为唯一的数字ID

**核心流程：BERT Model
1. Embedding
	- 模型接受`input_ids`，将数字ID转化为初始的，包含基础语义的向量
	- Embedding：**词嵌入（Token Embedding）、位置嵌入（Positional Embedding）、句子类型嵌入（Segment Embedding）** 这三个向量相加，形成每个词元最原始、包含所有必备信息的输入向量->`hidden_states`。
2. Encoder
	- 初始化后的`hidden_states`送入编码器，Encoder Block 由多个**Encoder Layer**组成
	- 向量流入后逐层向上处理，每一层会让每个词向量表示融合更多、更丰富的上下文信息
3. Prediction Heads
	- 向量数据->EncoderLayer->`hidden_states`最后，每个词的向量都蕴含了对整个句子的深刻理解
	- `Prediction_heads`属于预测头，这是一个小型的“完形填空”**MLM**网络，内部为`linear->激活函数->Linear->output`的过程就是一个`Prediction_heads`预测头结构。
![](inbox/Pasted%20image%2020250902215431.png)

拆解内部：**EncoderLayer**：
1. Attention
	- `hidden_states`流入Attention模块，所有词的向量互相交换信息，输出的`hidden_states`是**全局信息**交换
2. Intermediate/FNN
	- 前馈神经网络，每个词汇**单独**进行**非线性和特征**提取
3. “+”残差连接-Add&Norm
	- 图中`residual`进行了Attention&FNN前的复制回传。保证了梯度顺畅回传。

![](inbox/Pasted%20image%2020250902215706.png)
拆解内部：**Attention**：
1. 生成Q，K，V：`hidden_states`进行线性变换，分出`query_states` (Q), `key_states` (K), `value_states` (V)
2. 计算注意力分数：`query` 和 `key` 进行 `QK^T` (点积)=Score（此处还有一步`sqrt(d_K)`)
3. 分数归一化：`soft_max`函数，将Score调整到\[0，1\]、总和为1的注意力权重->`attention_weight`。此处形成了：**每个词分配多少“注意力”**
4. 加权求和：`attention_weight`于`valur_states`进行矩阵相乘。最终输出`attention_out`。

![](inbox/Pasted%20image%2020250902220707.png)


**总结BERT**：
“深度的双向语言理解器”->堆叠Transformer Encoder0>实现MLM与NSP预训->实现上下文深刻理解。
建立了“预训练+微调”范式，在[4. ELMo](../chapter1/第一章%20NLP基础概念.md#4.%20ELMo)的基础上更进一步。

BERT是预训练的开始，之后延伸出了MacBERT、RoBERTa、ALBERT、DeBERTa。

### 2. RoBERTa
Robustly Optimized BERT Approach--Facebook AI（**扩大训练参数**）
鲁棒优化的BERT方法

RoBERTa系统性研究了BERT的训练过程，找到提升性能的方法。新的一个模型结构，实现了：**训练方法和细节，与模型架构本身同样重要，甚至更重要。**

**核心优化4个优化点**：
**优化一**： 训练数据与时长
- BERT：使用了16GB文本数据（英文维基百科+图书预料库）进行训练
- RoBERTa：使用了160GB的高质量文本数据，在BERT上增加了预料（大量新闻、网页预料）。数据量是BERT的10倍。同时，训练步数和批大小（Batch Size）也远超BERT。

**优化二**：取消NSP任务（制作好一个核心）
- BERT：MLM+NSP两个任务
- EoBERTa：NSP任务过于冗余（原因：只能学习句子之间的表面特征，而不是真正的逻辑推理能力）
- 做法：去掉NSP任务，只保留了MLM，不去刻意划分句子，而是直接1个/n个文档连续抽取512个词元进行文本块训练。
- 作用：不去区分A/B句子是否相连，只专注于MLM，实现了上下文更加深刻的理解。

**优化三**：动态掩码Dynamic Masking（训练方式多样化）
- BERT：采用静态掩码Static Masking。数据预处理阶段，一句话中哪些词被`[Mask]`掩盖，都是一次性确定不再变化。这导致训练过程，模型看到同一句话，永远都是同一个词被掩盖。
- RoBERTa：采用动态掩码Dynamic Masking。预先生成`[Mask]`的数据集，而是每次向模型输入一个序列，实时、随机地生成一个新的掩盖模型。
- 作用：每次“完形填空”都不一样，**增强了语言泛化能力**。

**优化四**：Byte-Level BPE（更大的词汇表）
- BERT：基于**字符**的Byte-Pair Encoding词汇表，大小约3万。
- RoBERTa：基于**字节**的BPE，词汇表大小约5万，能够处理未登录词问题。

BPE是Tokenizer的一种处理方式：
- 基于字符：
	- 流程：一句话->拆分为基础字符->按照BPE算法，讲`t`和`h`此类合并为`th`的字符块，最后达到预设的字符数量，e.g. 3万。
	- 缺点：原有字符预料库内不存在的字符，emoji😂，导致其无法识别，被贴上`[UNK]`，信息丢失
- 基于字节：
	- 基础：计算机记录字符，都是使用256基础字节序列`000001111`此类
	- 流程：所有文本->转回基础字节序列->在字节序列上，去和BPE算法结合，把`t`和`h`的字节序列合并。
	- 核心：语料库解决了不存在字符无法识别的问题，都是计算机的字节序列。实现万物可以分词。
优秀的模型架构基础上：**更大规模数据、更长时间训练、更优化的训练策略**

| 对比维度        | 基于字符的BPE (BERT)                                      | 基于字节的BPE (RoBERTa/GPT)                                      |
|-----------------|----------------------------------------------------------|------------------------------------------------------------------|
| 基础单位        | 语料库中的所有字符                                       | 全宇宙统一的256个字节                                            |
| 未知词 (OOV)    | 遇到不在初始字符集里的字符时，会产生 [UNK] ，丢失信息。   | 永不产生 [UNK] ，能编码任何字符串，不丢失信息。                   |
| 鲁棒性          | 对多语言、表情符号、错别字、噪音文本的处理能力较差。       | 极强，真正做到了语言无关 (language-agnostic)，对各种文本形式都非常鲁棒。 |
| 词汇表大小      | 初始字符集可能很大（比如中日韩文），最终词汇表也较大。     | 基础词汇表极小 (256)，可以从一个非常小的基础构建出一个同样大小但表达能力更强的词汇表。 |
### 3. ALBERT
A Lite BERT（**减小训练参数**）

思想：减少参数量、保持模型性能：
- 词嵌入层+大规模参数->低维度的词嵌入+投影层
- N层的Self-Attention+FNN
作用：降低训练成本、内存占用、增快推理速度。

**核心两个优化点+一个新Task：**
**优化点一**：词嵌入矩阵分解 (Factorized Embedding Parameterization)
- BERT：词嵌入矩阵的大小为`V*H`,其中`V`上词汇表大小（比如3000），`H`为模型隐藏层大小（比如768）。这个矩阵参数量非常大（3000 * 768）。BERT的处理是把“单词的独立语义”和“准备送入Transformer的上下文语义”耦合在一个大矩阵进行训练学习。
- ALBERT：将巨大的矩阵分解为两个小矩阵
	- 先学习低维度的词嵌入矩阵`V*E`，其中`E`远小于`H`（比如128）。该矩阵负责学习每个单词的、与上下文无关的“基础含义”。
	- 通过投影层`E*H`将E放大到Transformer需要的H维上
- 作用：词嵌入层参数量降低80%

**优化点二**：跨层参数共享 (Cross-layer Parameter Sharing)
- BERT：N层的EncoderLayer，每一层都有自己独立的一套参数
- ALBERT：EncoderLayer共享同一套参数。模型只学习一组Attention参数和一组FNN参数，然后处理文本时，将这组参数重复使用12次。
- 作用：Encoder编码器参数量爆减1/N。

**新的工作：** 句子顺序预测(Sentence-Order Prediction, SOP)
- NSP问题：负样本（NotNext）是从其他文档随机抽取，与当前句子完全不相关，不利于模型学习连贯性，且冗余
- SOP做法：
	- 正样本：从语料库中选取连续的两个句子，顺序不变
	- 负样本：将这两个连续句子，**顺序颠倒**
- 作用：负样本与正样本来自同个文档，分类上一致。强迫模型学习更深层次的**语篇连贯性 (Discourse Coherence)**。

权衡：
- 参数效率高
- 训练和推理速度慢：时间换空间（参数流过12层）

### 总结


## 二、Encoder-Decoder PLM
---

### 1. T5
T5核心：万物皆可转换为“文本到文本”的问答
在此之前有：BERT+分类头、一个完整的翻译模型、一个摘要生成模型

T5的名字本身就是它的核心思想：**T**ext-to-**T**ext **T**ransfer **T**ransformer （文本到文本迁移Transformer）。
在T5之前，任务范式是：
- **输入**：文本序列
- **模型**：BERT
- **输出**：根据任务改变模型“头部”，输出可能是**一个类别标签**（分类），或者是**一段文本的起止位置**（问答）。

T5改进，任务范式：
- **输入**：文本序列（包含任务描述）
- **模型**：T5
- **输出**：**永远是文本序列**

这种“Text-to-Text”框架，意味着我们可以用同一种模型、同一种训练方式、同一种解码方式，来解决几乎所有的NLP任务。



![](inbox/Pasted%20image%2020250902234237.png)

**T5 Model**：经典的**Encoder-Decoder**架构
1. Decoder Layers：生成文本
2. Encoder Layers：输入预料
3. Embedding层：文字转换为模型能理解的向量

**Tokenizer&Embedding**：
1. Text（文本输入）：输入带“任务前缀”的文本，例如：`summarize: [一篇长文章...]`。
2. Tokenizer->input_ids（ID化）：Tokenizer文本转换成数字ID序列
3. Embedding（向量化）：`input_ids`转换为初始的词向量`hidden_states`。词向量包含基础语义。

**EncoderLayers**：
初始的`hidden_states`首先被送入右侧的**EncoderLayers**（编码器堆栈）。
- 核心组成：Self-Attention+FNN
- 数据流向：`LayerNorm -> Attention -> （残差连接） -> LayerNorm -> LayerFF -> （残差连接）`。
- Pre-LN：T5采用的结构。即将进入Self-Attention+FNN前，进行了LayerNorm（RMSNorm）。有助于增加训练稳定性。

**DecoderLayers**：
数据流入**DecoderLayers**（解码器堆栈），结构复杂一些
- Mask Self-Attention
- Cross-Attention：生成每一个词，回顾上一个+Encoder
- FNN，特征提炼

**底层模块拆解**
FNN（LayerFF）：
`LayerNorm -> Linear -> Gelu(激活函数) -> Linear -> dropout`

Attention+T5的核心调整Position Bias
- T5采用了一种更动态的方式。它**不**在输入端添加位置编码。而是在计算注意力分数时，直接给`QK^T`的结果加上一个叫做`position_bias`的“偏置项”。
- 这个偏置项是一个可学习的参数，它取决于Query和Key之间的**相对距离**。比如，模型可以学会“当两个词相邻时，给它们的注意力分数+5分”，“当一个词关注它前面第3个词时，给注意力分数+2分”。这种**相对位置偏置 (Relative Position Bias)** 被认为比绝对位置编码更灵活、更有效。

RMSNorm（T5的LayerNorm），通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。
![](inbox/Pasted%20image%2020250902234841.png)
有利于确保权重规模不会因模型过大过小而发生变化。

![](inbox/Pasted%20image%2020250903000203.png)

## 三、 Decoder-Only PLM
---
Decoder-Only PLM最火->GPT系列模型->LLaMA模型->GLM

Decoder-Only PLM，专精于生成下一个字，不回头。
核心思想：为“生成”而生的自回归模型
目的：生成文本 (Text Generation)

**其核心为：**
- **单向注意力 (Unidirectional Attention) / 因果注意力 (Causal Attention)**：这是它与BERT双向注意力的根本区别。严格的从左到右的信息流，因此被称为“因果”的，即前面的“因”导致后面的“果”。
- **自回归 (Autoregressive)**：用自己预测的结果，作为下一步预测的输入。

### 1.GPT
Generative Pre-Training Language Model
GPT坚持使用“预训练+微调思想”，其逻辑是：在海量无监督语料上预训练，进而在每个特定任务上进行微调，从而实现这些任务的巨大收益。
但是没能轰动BERT，直到通过不断扩大预训练数据、增加模型参数，GPT架构优化之后，发布了GPT-3，划下了LLM时代。

#### 1.1 模型架构（Decoder Only）
![](inbox/Pasted%20image%2020250903001338.png)
整体架构GPT model
- Tokenizer&Embedding（输入与嵌入）：Text->`input_ids`->`Embedding`初始化向量
- Positional Embedding（位置编码）：同Transformer一样。位置编码向量+词向量
- 核心：Decoder（纯解码器堆栈）：`hidden_states`直接进入一个由N个`DecoderLayer`堆叠而成的。
- Linear->output（输出）：经过N层`DecoderLayer`加工，最后的`hidden_states`流入`Linear`层（语言模型头），把向量映射到这个词汇表的大小，通过Softmax，去预测“下一个词”的概率分布，从而生成。

DecoderLayer
- **最关键的区别**：GPT的DecoderLayer**完全没有T5 DecoderLayer中的第二个Attention模块（交叉注意力）**。
- 原因：交叉注意力的作用是与Encoder进行信息交互，而GPT根本没有Encoder。

MLP（多层感知机，FFN）
- 与FNN区别：没有进行全连接提取特征，而是使用了两个一维卷积核来提取。
#### 1.2 预训练任务
Decoder-Only模型结构适用于文本生成。因此预训练任务也为因果语言模型(Causal Language Model, CLM)

Causal：
强调语言的“因果性”逻辑+时间上的“单向流”。符合人类写作的的信息单向流动自然规律。

CLM 可以看作 N-gram 语言模型的一个直接扩展。

**CLM与Decoder-Only架构的组合**
- **任务的要求**：CLM要求模型必须“只能向后看”。
- **架构的特性**：Decoder模块中的**Causal Mask**，从物理层面（或者说，从矩阵计算层面）完美地、强制性地实现了“只能向后看”这个要求。
- **巨大的优势：预训练和下游任务的无缝衔接**。

CLM对比MLM（BERT）

| 对比维度         | CLM (因果语言模型) - GPT 类模型                                | MLM (掩码语言模型) - BERT 类模型                        |
| ------------ | ----------------------------------------------------- | ---------------------------------------------- |
| ​**核心比喻**​   | 文字接龙大师 (生成者)                                          | 完形填空侦探 (理解者)                                   |
| ​**信息视角**​   | 单向 (Unidirectional)，只能看上文                             | 双向 (Bidirectional)，能看上下文                       |
| ​**任务目标**​   | 预测序列中下一个词                                             | 预测序列中被遮蔽（挖掉）的词                                 |
| ​**天然优势**​   | 文本生成 (NLG - Natural Language Generation)              | 文本理解 (NLU - Natural Language Understanding)    |
| ​**绝配架构**​   | Decoder-Only                                          | Encoder-Only                                   |
| ​**训练目标**​   | 自回归语言建模 (Autoregressive Language Modeling)，给定前文预测下一个词 | 掩码语言建模 (Masked Language Modeling, MLM)，预测被遮蔽的词 |
| ​**代表模型**​   | GPT系列 (GPT-1/2/3/4)                                   | BERT, RoBERTa, ALBERT                          |
| ​**典型应用场景**​ | 文本生成、对话系统、代码补全、创意写作                                   | 文本分类、情感分析、命名实体识别、问答系统                          |
| ​**输出形式**​   | 下一个词的概率分布，用于连续生成                                      | 每个词的上下文表示向量，用于下游任务分类或理解                        |

#### 1.3 GPT系列模型的发展历程
GPT坚定走“体量即正义”的思路

|模型|Decoder Layer|Hidden_size|注意力头数|注意力维度|总参数量|预训练语料|
|---|---|---|---|---|---|---|
|GPT-1|12|3072|12|768|0.12B|5GB|
|GPT-2|48|6400|25|1600|1.5B|40GB|
|GPT-3|96|49152|96|12288|175B|570GB|
**GPT-1：开山之作，生不逢时**
- **定位**：第一个证明Decoder-Only架构 + CLM预训练的模型。
- **实力**：参数量和数据量与当时的BERT-base相当。
- **困境**：在“阅读理解”（NLU任务）的年代环境下，BERT的双向理解能力在小体量下优势巨大，导致GPT-1的生成潜力没有被充分认识，光芒完全被BERT盖过。

 **GPT-2：崭露头角，理念超前**
- **核心变化**：**1. 体量剧增；2. 理念革新。**
- **体量剧增**：参数量和数据量都提升了一个数量级。
- **技术细节 (Pre-Norm)**：
    - 原始Transformer (Post-Norm): `计算 -> 残差连接 -> LayerNorm`
    - GPT-2 ( **Pre-Norm** ): `LayerNorm -> 计算 -> 残差连接`
    - **原因：** 当模型堆叠到几十层这么深时，Post-Norm的梯度很容易爆炸或消失，导致训练不稳定。Pre-Norm通过将Normalization前置，确保了数据流在深层网络中更平稳，让训练更深、更大的模型成为可能。
- **零样本学习 (Zero-shot)**：它试图摆脱“预训练-微调”的范式，不再为每个任务都准备成百上千的标注数据去“应试训练”。直接听懂“指令”就去完成任务。    
    - **旧范式**：给模型1000条电影评论和对应标签，微调它，让它学会情感分类。（感觉和Encoder需要做的任务更加倾斜）
    - **结果**：虽然当时效果还不够惊艳，但这个“让模型直接解决通用任务”的理念，为后来的发展指明了方向。

**GPT-3：力大砖飞，开启时代**
- **核心变化**：**1. 极致的体量；2. 涌现的能力；3. 范式的飞跃。**
- **极致的体量**：参数量和数据量再次提升了两个数量级。这种规模，使其正式成为第一个真正意义上的“**大语言模型 (LLM)**”。*模型结构上采用了“稀疏注意力”，为了在巨大的参数中，节约计算量的一种工程优化。*
- **涌现的能力 (Emergent Abilities)**：当模型规模跨越某个阈值后，会突然获得很多没有被直接训练过的能力，比如数学推理、代码生成、比喻理解等。
- **范式的飞跃 (Few-shot / In-context Learning上下文学习)**：虽然Zero-shot对模型要求太高，但只要在Prompt里给模型看Few例子，其理解能力就会变强。
    - 作用：不再需要去收集和标记数据，只需要设计好示例的Prompt，就能引导模型完成任务。**此类原理降低了AI使用门槛。**

```
zero-shot：请你判断‘这真是一个绝佳的机会’的情感是正向还是负向，如果是正向，输出1；否则输出0

few-shot：请你判断‘这真是一个绝佳的机会’的情感是正向还是负向，如果是正向，输出1；否则输出0。你可以参考以下示例来判断：‘你的表现非常好’——1；‘太糟糕了’——0；‘真是一个好主意’——1。
```

### 2. LLaMA
Meta设计的目前最普适的LLM架构

核心思想：**在同等计算预算下，用海量的、更高质量的数据去训练一个相对较小的模型（比如7B或13B），其最终效果可以超过训练数据不足的、更大的模型**。
工作重心转移：从“单纯追求更大的模型参数”，部分转移到了“追求更大、更高质量的数据集”以及“更高效的模型架构”上。

#### 2.1 模型架构
![](inbox/Pasted%20image%2020250903005831.png)
**三大关键调整与优化**
**优化一**： LlamaRMSNorm
- DecoderLayer中的中心化、归一化LayerNorm的一次调整
- Root Mean Square Norm：简化的LayerNorm，去除了（减去均值）的步骤。只进行归一化。
- 目的：Transformer架构中，去除中心化不影响性能，却能提升计算速度。**极致的效率**

**优化二**：RoPE旋转位置编码
- Transformer：位置向量**加上**词向量。T5是注意力分数加个偏置。
- Rotary Positional Embedding：**乘法**形式的位置编码
	- 原理：旋转置入位置信息给Q、K，让Q、K向量指针在空间中旋转。
	- 作用：使单词之间的注意力分数，**只与相对位置有关系**。

**优化三**：SwiGLU前馈网络
- FNN：`Linear`层->`ReLU/GELU`激活函数->`Linear`层
- SwiGLU：`x`分为两部分各自进入`gate_proj`和`up_proj`。一路通过了激活函数，再与另外一路相乘，起到门控作用。
	- 原理：通过`gate_proj`去动态调整算出`up_proj`其中哪一部分内容信息的重要/保留/不重要
	- 作用：门控机制，让网络更强的表达能力，提升模型的性能

#### 2.2 LLaMA模型的发展历程
 **LLaMA 1 (2023年2月)**
- **发布**：从7B到65B参数的系列模型。其性能惊人（13B模型在很多任务上超越了GPT-3的175B），以及可以在消费级硬件上运行的可能性。    
- **意义**：**奠定了开源LLM的技术基石**。无数重要的开源社区项目（如Alpaca, Vicuna）都是基于LLaMA 1进行指令微调的。

 **LLaMA 2 (2023年7月)**
- **发布**：Meta发布的官方迭代版本，**开源且可免费商用**，这是其最重要的贡献。
- **核心改进**：
    1. **更大的数据**：在2万亿（2T）个Token上进行了训练，数据量远超一代。
    2. **更长的上下文**：支持4096的上下文长度，是一代的两倍。
    3. **为对话优化**：发布了`Llama-2-chat`模型，通过指令微调和人类反馈强化学习(RLHF)进行了专门的优化，对话能力大幅提升。
    4. **架构微调(GQA)**：图中`Attention`模块里的`repeat_kv`暗示了LLaMA 2中的一项重要优化——**分组查询注意力(Grouped-Query Attention, GQA)**。简单来说，它允许多个Query头共享同一组Key和Value头，极大地减少了生成过程中的内存占用，使得处理长文本时的推理速度更快。

**LLaMA-3 系列（2023年4月）**
- **发布**：Meta发布了LLaMA-3，包括8B和70B两个参数量版本，同时透露400B的LLaMA-3还在训练中。
- LLaMA-3支持**8K长文本**，并采用了编码效率更高的tokenizer，词表大小为128K。
- 使用了超过15T token的预训练语料，是LLaMA-2的7倍多。

### 3. GLM
智谱开发的中文LLM之一，包括 ChatGLM1、2、3及 GLM-4 系列模型，覆盖了指令理解、代码生成等多种应用场景。

内部架构实现了注意力可切换模式。

#### 3.1 模型架构-相对于GPT的略微修正
**核心思路：是在传统 CLM 预训练任务基础上，加入 MLM 思想。** 从而构建一个在 NLG 和 NLU 任务上都具有良好表现的统一模型
整体模型结构为Decoder-Only结构，其三点差异为：
- Post Norm 而非 Pre Norm。
	- 先完成残差再进行LayerNorm。
	- 参数正则化效果更强，模型鲁棒性也更好。并且防止了梯度爆炸问题。
- 单个线性层去实现最终token的预测，而不是用MLP。
	- 也是增强了鲁棒性。
	- 减少了最终输出的参数量，将更大的参数量放在模型本身。
- 激活函数：ReLU->GeLUs
	- 保证量激活后的非线性输出，具备一定的连续性
	- ![](inbox/Pasted%20image%2020250903013128.png)

#### 3.2 预训练任务-GLM
**自回归空白填充 (Autoregressive Blank Infilling)**：
- Blank Infilling（文本破坏）
	- 原始序列文本->随机挖走部分内容，把文本切成几个部分->PartA（剩下的文本）、PartB（挖走的文本）
- Autoregressive（重组）
	- 让PartA与PartB重组在一起，并且用`[Mask]`分隔
- 新的注意力机制（重组的目的）
	- 模型目的，根据PartA**自回归**生成PartB
	- 使用特殊的注意力机制：**2D混合掩码**
		- PartA内部：单词为双向注意力
		- PartB内部：单向自回归注意力
		- PartB看PartA：可以完全获取A
		- PartA看PArtB：A完全看不到B，去预测B
- 作用：在同一个预训练任务中，同时学会了**BERT式的双向上下文理解**和**GPT式的单向文本生成**。

它的模型架构本质上是一个Transformer解码器堆栈，但通过这个灵活的预训练任务，兼具了编码和解码的能力，实现了真正的“通用语言模型”。

#### 3.3 GLM模型的发展历程
 **GLM (2021年)**
- 这是最初的论文，提出了上述的“自回归空白填充”核心思想。
- 在当时的NLU基准测试上，它用一套统一的模型，同时在理解类任务和生成类任务上取得了与BERT和T5相当甚至更好的成绩，证明了这个统一框架的巨大潜力。

 **GLM-130B (2022年)**
- 这是清华团队对“体量即正义”的一次成功实践，也是当时**亚洲最大的单体预训练模型**。
- 参数量达到**1300亿**，是一个强大的**中英双语**模型。
- 在架构上，为了训练如此巨大的模型，它采用了一些先进技术，如DeepNorm来保证训练稳定性。
- GLM-130B的开源（模型权重对学术研究开放），极大地推动了国内大模型技术的发展和研究。

 **ChatGLM 系列 (2023年至今)**
ChatGLM系列是GLM思想走向大规模应用、并与开源社区紧密结合的产物。它们是基于GLM架构，面向对话进行优化的模型。
- **ChatGLM-6B (2023年3月)**：
    - 这是引爆中国开源LLM社区的**现象级模型**。
    - 它基于GLM-130B的技术，训练了一个60亿参数的中英双语对话模型，并且**完全开源**。
    - 最重要的是，它对硬件极其友好，普通消费级显卡（甚至CPU）就能运行，极大地降低了开发者和研究者接触、使用和“改装”大模型的门槛。
- **ChatGLM2-6B (2023年6月)**：
    - 是一次全面的升级。模型性能更强、上下文长度更长（支持32K）、推理速度更快。
    - 在架构上，它吸收了LLaMA的许多优点，比如使用**SwiGLU**、**RoPE**等，并采用了**多查询注意力(Multi-Query Attention, MQA)**，这是一种比LLaMA 2的GQA更极致的优化，进一步提升了推理效率。这体现了开源社区技术融合的趋势。
- **ChatGLM3-6B (2023年10月)**：
    - 最新的迭代版本。在继续提升基础模型能力的同时，重点强化了**工具调用（Function Calling）**和**Agent（智能体）**的能力。
    - 这意味着它不再仅仅是一个聊天机器人，而被设计成一个可以调用外部API（如搜索、计算器、订票）来完成复杂任务的“大脑”。
