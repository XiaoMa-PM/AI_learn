# 第一章 NLP基础概念
---

## 一、什么是NLP
---
NLP 是 一种让计算机理解、解释和生成人类语言的技术。
Task：核心任务是通过计算机程序来模拟人类对语言的认知和使用过程。
现存挑战：处理歧义、理解抽象概念、处理隐喻和讽刺

## 二、NLP发展历程
---
早期规则基础方法--统计方法--ML/DL
**早期探索阶段**：（1940--1960）
二战后，一种语言自动翻译成另外一种语言。艾伦·图灵--图灵测试
- 该阶段机器翻译系统：以来字典查找和基本词序规则翻译

**符号主义与统计方法** ：（1970--1990）
1970: 逻辑基础的范式和自然语言理解。该阶段研究在符号主义/统计方法。
- 符号主义：形式语言和生成语法
- 统计方法：统计和概率方法
1980: 统计模型开始取代复杂的“手写”规则

**机器学习和深度学习** ：（2000--）
深度学习模型如循环神经网络（Recurrent Neural Network，RNN）、长短时记忆网络（Long Short-Term Memory，LSTM）和注意力机制等技术被广泛应用于 NLP 任务中。
2013: Word2Vec模型提出开创了词向量时代，提供国家有效的文本表示方法
2018: BERT模型引领预训练语言模型新时代
Transformer模型，通过大量参数，实现生成高质量的文本

## 三、NLP任务
---

### 1. 中文分词（CWS）
Chinese Word Segmentation

中文不同英文有空格间隔，所以CWS是中文文本处理的首要步骤。
目的，连续中文文本Seg为有意义的词汇序列

注意：“，”“。”为单独Seg

正确分词作用：词性标注、实体识别、句法分析等

### 2. 子词切分
>Subword Segmentation

NLP领域一种常见的文本预处理技术，将词汇进一步分解为更小单元，即子词

子词作用：适用于处理词汇稀疏问题，理解未遇到的新词
核心是利用了英语单词**词根**

```
输入：unhappiness

不使用子词切分：整个单词作为一个单位，输出：“unhappiness”
使用子词切分（假设BPE算法）：单词被分割为：“un”、“happi”、“ness”
```

### 3. 词性标注（POS）
>Part-of-Speech Tagging

NLP领域一项**基础**任务
分配词性：英文的名词（Noun，N）、动词（Verb，V）、形容词（Adjective，Adj）等

作用：利于理解句子结构、进行句法分析、语义角色标注等高级NLP任务

作用于信息提取、情感分析、机器翻译
``**Question：那语意分析和这个的关系？**

词性标注使用了机器学习模型，隐马尔可夫模型HMM、条件随机场CRF或者深度学习的循环神经网络RNN、长短时记忆网络LSTM等。模型通过学习大量标注数据来预测新句子中每个单词的词性。

### 4. 文本分类
>Text Classification

NLP领域一项**核心**任务
将给定的文本自动分配到一个或多个预定义的类别中。
``Quesion：聚类同理？

文本分类关键在于理解文本的含义和上下文，并基于此文本映射到特定的类别。

```
# 新闻分类
文本：“NBA季后赛将于下周开始，湖人和勇士将在首轮对决。”
类别：“体育”

文本：“美国总统宣布将提高关税，引发国际贸易争端。”
类别：“政治”

文本：“苹果公司发布了新款 Macbook，配备了最新的m3芯片。”
类别：“科技”
```

**文本分类执行**的关键在于选择合适的**特征表示**和**分类算法**，高质量数据。

### 5. 实体识别（NER）
>Named Entity Recognition，命名实体识别

NLP领域的一项**关键**任务
自动识别文本中具有特定意义的实体，并将它们分类为预定义的类别

作用：用于信息提取、知识图谱构建、问答系统、内容推荐等，帮助系统理解文本中的关键元素及其属性

```
输入：李雷和韩梅梅是北京市海淀区的居民，他们计划在2024年4月7日去上海旅行。

输出：[("李雷", "人名"), ("韩梅梅", "人名"), ("北京市海淀区", "地名"), ("2024年4月7日", "日期"), ("上海", "地名")]
```

### 6. 关系抽取
>Relation Extraction

NLP领域的一项**关键**任务。
从文本中是识别实体之间的语义关系。
关系有：因果关系、拥有关系、亲属关系、地理位置关系等。

作用：用于理解文本内容、构建知识图谱、提升机器理解语言的能力等。

```
输入：比尔·盖茨是微软公司的创始人。

输出：[("比尔·盖茨", "创始人", "微软公司")]
```

### 7. 文本摘要
>Text Summarization

NLP领域的一项**重要**任务。
生成一段简介准确的摘要，概括原文的主要内容。

根据生成方式不同分为：抽取式摘要（Extractive Summarization）和生成式摘要（Abstractive Summarization）。

抽取式摘要：
- 原理：选取**原文**关键句子或词语组成摘要。
- 优点：原文信息，准确性高
- 缺点：容易出现不流畅
生成式摘要：
- 原理：不仅选取文本片段，还进行片段的重新组织和改写，并生成新的内容，需要深度理解原文，并且生成新的片段表达原有意思
- 优点：更加符合语意
- 缺点：需要更加复杂的模型，如：基于注意力机制的序列到序列模型（Seq2Seq）

作用：信息检索、新闻推送、报告生成等领域。可以实现快速获取文本的核心信息，节省阅读时间，提高信息处理效率。

### 8. 机器翻译（MT）
>Machine Translation

NLP领域的一项**核心**任务
将一种自然语言（源语言）自动翻译成另外一种自然语言（目标语言）的过程

不仅词汇转换，需要传达语言文本的语义、风格和文化背景等。最终实现准确和流畅的翻译。

基于神经网络的Seq2Seq模型、Transformer模型等，这些模型能够学习到源语言和目标语言之间的复杂映射关系。

### 9. 自动问答（QA）
>Automatic Question Answering

NLP领域的一项**高级**任务
理解自然语言问题，并且根据数据源自动提供准确的答案。模拟人类理解和回答问题能力，涵盖从简单的事实查询到复杂的推理和解释。

QA系统构建涉及：多个NLP子任务，如信息检索、文本理解、知识表示和推理等

自动问答大致可分为三类：检索式问答（Retrieval-based QA）、知识库问答（Knowledge-based QA）和社区问答（Community-based QA）。
- 检索式问答：通过搜索引擎等方式从大量文本中检索答案
- 知识库问答：通过结构化的知识库来回答问题
- 社区问答则依赖于用户生成的问答数据，如问答社区、论坛等

## 四、文本表示的发展历程
---
目的：人类语言转化为计算机可以处理的形式，文本数据数字化。
NLP领域的一项**基础和必要**工作。
**直接影响决定NLP系统的质量和性能。**

文本表示：文本中的语言单位（如字、词、短语、句子等）、单元关系、单元结构信息--转换-->计算机能处理的向量、矩阵、其他数据结构
作用于保留足够的语义信息，利于NLP任务，如：文本分类、情感分析、机器翻译等。

发展历程：规则方法-->统计学习方法-->深度学习技术-->
### 1. 词向量--向量空间模型（VSM）
>向量空间模型（Vector Space Model,VSM）

NLP领域一种基础强大的文本表示方法
实现复杂文本数据转换为易于计算和分析的数学形式。

将文本转化为高维空间的向量。模型中的**每个维度**代表一个**特征项**（可以是字、词、词组或者短语），而向量中的**每个元素值**代表该特征项在文本中的**权重**，权重计算通过特定计算公式来确定。

向量模型应用广泛，包括不限于文本相似度计算、文本分类、信息检索等NLP任务。
此外，通过矩阵运算如特征值计算、奇异值分解（singular value decomposition, SVD）等方法，可以**优化**文本向量表示。

存在问题：数据稀疏性和维数灾难。
原因：特征项数量庞大导致向量维度极高，同时多数元素值为零。

存在问题：忽略了文本中的结构信息，如词序和上下文信息
原因：模型基于特征项之间的独立性假设

不足：特征项选择、权重计算方法

VSM方法词向量：
```
# "雍和宫的荷花很美"
# 词汇表大小：16384，句子包含词汇：["雍和宫", "的", "荷花", "很", "美"] = 5个词

vector = [0, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ...]
#                    ↑          ↑          ↑          ↑          ↑
#      16384维中只有5个位置为1，其余16379个位置为0
# 实际有效维度：仅5维（非零维度）
# 稀疏率：(16384-5)/16384 ≈ 99.97%
```

> 词汇表是一个包含所有可能出现的词语的集合。在向量空间模型中，每个词对应词汇表中的一个位置，通过这种方式可以将词语转换为向量表示。例如，如果词汇表大小为 16384 ，那么每个词都会被表示为一个 16384 维的向量，其中只有该词对应的位置为 1，其他位置都为 0。

两个解决方案：
1. 改进特征表示方法：借助图方法、主题方法等进行关键词提取
2. 改进和优化特征项权重的计算方法，在现有基础上进行融合

### 2. 语言模型（N-gram）

NLP领域的一种基于统计语言模型
应用于语音识别、手写识别、**拼写纠错**、机器翻译和搜索引擎等众多任务。


**N的阶数含义**​：

| N-gram类型            | 依赖历史长度 | 概率公式             |
| ------------------- | ------ | ---------------- |
| ​**Unigram**​ (N=1) | 0（独立）  | `P(W) = ∏ P(wₖ)` |
| ​**Bigram**​ (N=2)  | 1个词    | `P(W) = ∏ P(wₖ）  |
| ​**Trigram**​ (N=3) | 2个词    | `P(W) = ∏ P(wₖ）  |
计算某个词在上一个词前面出现的概率

举例子：

| 词对           | 出现次数 |
| ------------ | ---- |
| (“我”, “爱”)   | 50   |
| (“爱”, “自然”)  | 30   |
| (“自然”, “语言”) | 25   |
| (“语言”, “处理”) | 20   |
| “我” 独立出现     | 100  |

优点：实现简单
缺点：N-gram模型忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息。
原因：N较大出现数据稀疏性问题，模型参数空间会急剧增大，相同的N-gram序列出现概率变得很低，导致模型无法有效学习，模型泛化能力降低

## 3. Word2Vec

词嵌入技术(Word Embedding)技术
基于神经网络NNLM语言模型，通过学习词与词之间的上下文关系生成词的密集向量表示。
核心思想是利用词在文本中的上下文信息来捕捉词之间的语义关系，从而使得语义相似或相关的词在向量空间中距离较近。

Word2Vec模型2种训练模型（两种学习方法）：
1. 连续词袋模型CBOW(Continuous Bag of Words)：
	- 任务：完形填空。
	- 过程：挖掉一句话里的一个词，根据目标词周围词（上下文）去预测中间这个被挖掉的词。
	- 学习方式：模型一开始瞎猜，猜错调整内部参数，经过成千万次这样的训练，能准确猜中词，迫使经常一起出现的词向量越来越相似。
2. Skip-Gram模型：
	- 任务：看词猜伙伴。
	- 过程：与CBOW相反。给模型一个词，它预测词周围（上下文）会出现什么词。
	- 学习方式：和CBOW一样，通过不断的预测和调整，最终学到优质的词向量。
3. 实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好。

相比传统高维稀疏，Word2Vec是低维密集向量。
Word2Vec模型能够捕捉到词与词之间的语义关系。

```
`vector('国王') - vector('男人') + vector('女人')` ≈ `vector('王后')`
```
向量捕获向量的性别属性，使用性别进行加减学习，从而理解未知的词

应用：
- **文本分类/情感分析**：句子里的词进行向量化，之后整合输入给一个分类模型
- **机器翻译**：
- **推荐系统**：根据检索的词向量，寻找接近的相关向量词，进行匹配推荐。
- **问答系统**：

局限性：**一词多义问题**

## 4. ELMo
Embedding from Language Models语境向量化，**解决了Word2Vec的局限性。** Word2Vec是“字典”，而ELMo是**语言专家**。

“动态”和“语境化”词向量：
动态（Dynamic）：做了**预训练模型**，你输入句子后，他会根据阅读上下文，去生成目标词的**专属的、定制化向量**。

```
I went to the **bank** to deposit money. (我去了银行存钱。)    
I sat on the river bank. (我坐在河岸上。)
vector('bank' in sentence 1) -> 这个向量会和 vector('money'), vector('deposit') 的意思更接近
vector('bank' in sentence 2) -> 这个向量会和 vector('river'), vector('water')的意思更接近。

```
该技术在于突破了**语境理解**，此外，很大的贡献是**预训练语言模型**（Pre-trained Language Model）

ELMo发展的延伸：
1. **“预训练+微调”**：ELMo 先在一个巨大的文本语料库上训练一个通用的、深刻理解语言的模型，然后再将这个模型应用到具体的下游任务（比如情感分析）。形成了Pre-training+Fine-tuning范式
2. 局限点Point：虽然他能理解上下文，但是理解超长文存在难度--即**LSTM（长短期记忆网络）存在局限 
3. 继任者**Transformer-BERT模型**：BERT使用了自注意力机制，并且是“双向”理解的（ELMo也是双向，BERT更加彻底）。OpenAI的GPT把这种模式推向极致。

ELMo的🧠**大脑**双向长短期记忆网络（bi-LSTM）
1. **LSTM**：“单向阅读器”，从左到右逐字阅读句子，读每个词，更新自己的记忆，记忆包含目前为止所有读过的内容
2. **Bi-directional**：ELMo不仅从左到右阅读，当需要理解某个词，就会去阅读到进行正方向阅读记忆：
- **正向阅读器：** 从 "I" 开始，一路读到句末。当它读到 "bank" 时，它的记忆里包含了 "I went to the" 的信息。
- **反向阅读器：** 从句末的 "." 开始，一路反向读到句首。当它读到 "bank" 时，它的记忆里包含了 ". money deposit to" 的信息。
1. 拼接上下文：对于“bank”，ELMo会把正向、反向阅读器在“bank”处的记忆拼接一起。生成饿向量就同时包含了“上文”和“下文”的学习，对“bank”有更加具体的把握。

实际上ELMo上多层的bi-LSTM，底层上语法结构理解，高层网络上复杂语义信息。

### 5. 总结
**NLP🧬进化链路：**
**静态词向量 (Word2Vec, GloVe) -> 动态/语境化词向量 (ELMo) -> 基于 Transformer 的更强语境化模型 (BERT, GPT)**

**问个不停：**
为什么Word2Vec是静态？
>因为其是固定理解每个词，并且形成固定的向量表达。

为什么ELMo是动态的？
>因为是“预训练模型+语境”，即先投喂语料库，无需标注词的属性（**无标注**），而是在Bi-LSTM上去打属性。
>核心在于是先理解语言规律后，去去根据语境定义向量。

为什么ELMo在理解超长文存在难度？
>ELMo原理是**循环神经网络RNN**是逐层传递信息（递话游戏）
>该逐层，当文本超长，会出现问题：长期记忆丢失、串行计算，效率低下
>长期记忆丢失：多层的时候，最初的隐藏层灰出现丢失（权重变弱）
>串行计算：无法并行处理，智能1->2->...->999->1000层方式计算。