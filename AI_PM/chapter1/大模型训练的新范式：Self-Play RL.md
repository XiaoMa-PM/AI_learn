# 大模型训练的新范式：Self-Play RL
时间截止：2024年10月

---
课程原因，GPT-o1，使用了强化学习
## 技术原理
---
### 两个关键词
**Self-Play**
- 自己生成内容、自己训练自己
- 在边界/规则足够清晰的任务下，由大模型随机完成任务，把好的拿出来用来更新模型
- 象棋是最典型的（明确的胜负条件，大量的过程**行为**数据），数学、物理都是（这恰好是大模型最欠缺的）

**RL=Reinforcement Learning**
- 在监督学习之外的训练方法，通过反馈来学习
- 已经比较通用的是RLHF（RL from Human Feedback）

## 一句话总结
- 让AI用随机的路径尝试新的任务（自己生成答案），如果效果超预期（符合强化学习的奖励条件，那就更新神经网络的权重，使得AI技术多使用这个成功的事件，再开始下一次的尝试。

### 新技术原因
- 能用的语料，基本拿来训练了；
- 模型参数已经达到临界值，再多也没啥用了
- 一些基本定理，只有结果，没有过程数据
	- 大模型需要过程数据，来训练自己的“认知”（学会推理？根据已知信息猜出答案的过程）
	- 大模型需要足够多的推理路径，找到“正确”的规律（还是概率？）
- 学而不思则惘，思而不学则殆
	- 光找知识的规律没用，需要思考
	- 光思考也没用，要有足够多的案例来验证思考
- 延伸阅读：[全网最全 OpenAI o1 万字综述：创新、原理和团队 \| 人人都是产品经理](https://www.woshipm.com/it/6118783.html)



## 应用价值
---
### 大量“过程“数据的学习，大模型可能会掌握”自我验证“能力
- 过去的大模型不存在验证上一个大模型的产出是不是正确（都是概率，都对、也都错）
-  当大模型具备“自我验证”能力，在某种意义上就具备了推理能力（**可以“停下来”绅士自己刚刚产出的内容，而不是“闭眼一往无前”**） #Question 这句话对吗

### 「幻觉」问题可能会消失
- 原始的监督式学习，尤其是无监督学习，大模型可能在“自学”过程中总结出一些“自圆其说，但是不对”的知识，这可能就是幻觉的来源（另一部分是RLHF过程中，那些给大模型反馈的人不行...） #Question 太过于绝对了“来源”一词
- 强化学习，尤其是Self-play Rl，因为是在有严格对错答案的反馈中学到知识，学习过程指标的过程中，大模型或许会掌握一些“对错判断”能力，来修正过去的参数权重
### 01模型中内置CoT，所有大模型都会用，但因为他们不具备推理能力，所以都是”形式主义“

## 作业
---
分别给出3个场景
- 哪些场景，具备增强推理能力的模型更好？
- 哪些场景，普通模型更好？
