# ç¬¬äºŒç«  Transformeræ¶æ„
---

## ä¸€ã€æ³¨æ„åŠ›æœºåˆ¶
---

### 1. ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶
Word2Vecå•å±‚ç¥ç»ç½‘ç»œ-->ç¥ç»ç½‘ç»œ
è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰CVå‘å±•èµ·æº**ç¥ç»ç½‘ç»œæ ¸å¿ƒä¸‰ç§æ¶æ„ï¼š**
- å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼ŒFNNï¼‰
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼ŒCNNï¼‰
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰ï¼Œèƒ½å¤Ÿä½¿ç”¨å†å²ä¿¡æ¯ä½œä¸ºè¾“å…¥ã€åŒ…å«ç¯å’Œè‡ªé‡å¤çš„ç½‘ç»œï¼š![](inbox/Pasted%20image%2020250823024029.png)

**RNNåŠLSTMå…·æœ‰æ•æ‰æ—¶åºä¿¡æ¯ï¼Œä½†æ˜¯å­˜åœ¨ä¸¤ä¸ªç¼ºé™·**ï¼š
- é•¿è®°å¿†ä¸¢å¤±ï¼šåºåˆ—æŒ‰é¡ºåºè¯»å…¥ï¼Œè·ç¦»å˜é•¿ã€å†…å­˜æœ‰é™
- ä¸²è¡Œè®¡ç®—ï¼Œæ•ˆç‡ä½ä¸‹ï¼šæ— æ³•å¾ˆå¥½åˆ©ç”¨GPU

åˆ©ç”¨äº†CVçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰ï¼Œåœ¨NLPé¢†åŸŸåšå‡ºäº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œ--Transformerï¼Œæˆä¸ºLLMã€æ·±åº¦å­¦ä¹ æœ€æ ¸å¿ƒçš„æ¶æ„ä¹‹ä¸€ã€‚

**æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰ï¼š**
å¤§è„‘ğŸ§ æœªæ¥ç†è§£å½“å‰ç›®æ ‡ï¼Œå¯¹æ‰€æœ‰è¾“å…¥ä¿¡æ¯è¿›è¡Œæ‰«æï¼Œåˆ†é…ä¸åŒæ³¨æ„åŠ›åˆ†æ•°çš„èƒ½åŠ›ï¼Œå°±æ˜¯**æ³¨æ„åŠ›æœºåˆ¶ã€‚**

**ç®€åŒ–çš„å·¥ä½œæµç¨‹ï¼ˆQï¼ŒKï¼ŒVæ¨¡å‹ï¼‰**
Queryï¼ŒKeyï¼ŒValue

Queryï¼šæŸ¥è¯¢ç›®æ ‡
Keyï¼švectorï¼ˆâ€œâ€œç”µè„‘â€çš„ç´¢å¼•/æˆ–è€…æ¦‚æ‹¬â€ï¼‰ï¼Œç´¢å¼•æ ‡ç­¾ã€‚**è®¡ç®—ç›¸å…³æ€§ï¼Œå¸å¼•æ³¨æ„åŠ›ï¼Œä¸Qè®¡ç®—â€œæ³¨æ„åŠ›åˆ†æ•°â€ã€‚** 
Valueï¼švectorï¼ˆâ€œç”µè„‘â€ï¼‰ï¼Œå†…å®¹æœ¬èº«ã€‚**è¢«æå–ä½¿ç”¨ï¼Œæä¾›å†…å®¹ï¼Œä¸â€œæ³¨æ„åŠ›åˆ†æ•°â€å°±è¡ŒåŠ æƒæ±‚å’Œå¾—å‡ºç»“æœã€‚** 

çœŸå€¼ï¼ˆGround Truthï¼‰ï¼šæ¨¡å‹è®­ç»ƒçš„ç›®æ ‡ç­”æ¡ˆ

åœ¨**è®­ç»ƒ**ä¸€ä¸ªç¿»è¯‘æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å†…éƒ¨çš„**æ³¨æ„åŠ›æœºåˆ¶**ä¼šåˆ©ç”¨ **Key** å’Œ **Value** å»å¤„ç†è¾“å…¥çš„å¥å­ï¼Œå¾—å‡ºä¸€ä¸ªç¿»è¯‘ç»“æœï¼ˆé¢„æµ‹ï¼‰ã€‚ç„¶åï¼Œæ¨¡å‹ä¼šæŠŠè¿™ä¸ªç»“æœå’Œæ•°æ®é›†é‡Œçš„**çœŸå€¼ï¼ˆGround Truthï¼‰è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœå‘ç°æœ‰å·®è·ï¼Œæ¨¡å‹å°±ä¼šè°ƒæ•´è‡ªå·±å†…éƒ¨çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬é‚£äº›ç”¨æ¥ç”Ÿæˆ  Key** å’Œ **Value** çš„ç½‘ç»œå±‚ï¼Œä»¥ä¾¿ä¸‹æ¬¡èƒ½åšå‡ºæ›´æ¥è¿‘**çœŸå€¼**çš„é¢„æµ‹ã€‚

### 2. æ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶
å¦‚ä½•è®¡ç®—å¾—å¤„æ³¨æ„åŠ›åˆ†æ•°ï¼Ÿ

 **Step1:æµ‹é‡ç›¸ä¼¼åº¦ï¼Œè®¡ç®—å•ä¸ªè¯Score**
è¯å‘é‡è¡¨ç¤ºè¯­ä¹‰ä¿¡æ¯ï¼Œè·ç¦»è¿œè¿‘è¡¨ç¤ºè¯ä¹‰æ¥è¿‘ã€‚é€šè¿‡æ¬§å¼è·ç¦»æ¥è¡¡é‡è¯å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œæ­¤å¤„ä½¿ç”¨ç‚¹ç§¯è®¡ç®—ã€‚
$$
vÂ·w = \sum_{i}v_iw_i
$$
è¯­ä¹‰ç›¸ä¼¼ï¼šç‚¹ç§¯>0ï¼Œè¯­ä¹‰ä¸ç›¸ä¼¼ï¼šç‚¹ç§¯<0ã€‚

**Step2:è®¡ç®—ä¸€ç³»åˆ—çš„åŸå§‹Score**
è®¡ç®—Queryä¸æ‰€æœ‰è¯çš„Keyå‘é‡è¿›è¡ŒçŸ©é˜µç‚¹ç§¯è®¡ç®—ï¼Œå¾—åˆ°xä¸ºQä¸æ‰€æœ‰è¯Keyï¼ˆåŒ…å«è‡ªå·±ï¼‰çš„ç›¸å…³æ€§ã€‚æ­¤å¤„æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œè€Œéæ±‚å’Œç»“æœã€‚
$$
x = qK^T
$$
```
**ä¸¾ä¾‹ï¼š** å‡è®¾æˆ‘ä»¬è¦ç†è§£å¥å­ "The cat sat" ä¸­ "cat" è¿™ä¸ªè¯ã€‚
Q_cat ä¼šåˆ†åˆ«å’Œ K_Theã€K_catã€K_sat è¿›è¡Œç‚¹ç§¯ã€‚
å¾—åˆ°ä¸‰ä¸ªåŸå§‹åˆ†æ•°ï¼š
e1 = Q_cat \cdot K_The (æ¯”å¦‚å¾—åˆ° 12.5)  
e2 = Q_cat \cdot K_cat (æ¯”å¦‚å¾—åˆ° 30.8)
e3 = Q_cat \cdot K_sat (æ¯”å¦‚å¾—åˆ° 25.1)

è¿™äº›åˆ†æ•° `[12.5, 30.8, 25.1]` ç›´è§‚åœ°åæ˜ äº†ç›¸å…³æ€§ï¼Œåˆ†æ•°è¶Šé«˜ï¼Œç›¸å…³æ€§è¶Šå¼ºã€‚
```

**Step3:ç¨³å®šè®­ç»ƒæŠ€å·§â€”â€”ç¼©æ”¾**
åŸå› ï¼šæ‰€æœ‰å‘é‡çš„dç»´åº¦å¾ˆé«˜ï¼Œç‚¹ç§¯è®¡ç®—ç»“æœä¼šç‰¹åˆ«å¤§ï¼Œå¯¼è‡´è¿‡å¤§æ ‘æå¯¹äºåç»­çš„ï¼ˆSoftmaxï¼‰å¸¦æ¥å›°éš¾ï¼Œå¯¼è‡´æ¢¯åº¦ï¼ˆæ¨¡å‹å­¦ä¹ ä¿®æ­£ä¿¡å·ï¼‰å˜å¾—æå°ï¼Œä¸åˆ©äºæ¨¡å‹è®­ç»ƒ
è§£å†³æ–¹æ¡ˆï¼šå¢åŠ ç¼©æ”¾å› å­
$$ \sqrt{d_k} = \sqrt{64} = 8 $$
dkä¸ºkçš„ç»´åº¦
æŠŠä¸Šä¸€éƒ¨è®¡ç®—çš„e1ã€e2ã€e3è¿›è¡Œscaledã€‚
```
e1_scaled = 12.5 / 8 = 1.56
e2_scaled = 30.8 / 8 = 3.85
e3_scaled = 25.1 / 8 = 3.14
```
ä¸ºäº†åˆ©äºæ¨¡å‹ç¨³å®šå­¦ä¹ 

**Step4:è½¬æ¢ä¸ºæœ€ç»ˆæƒé‡--Softmaxå‡½æ•°/å½’ä¸€åŒ–å¤„ç†**
æœ€ç»ˆç»“æœç›®çš„æ˜¯â€œæ³¨æ„åŠ›æƒé‡â€ï¼Œå¿…é¡»æ»¡è¶³**æ­£æ•°**å’Œ**æ‰€æœ‰æƒåŠ èµ·æ¥ä¸º1**ã€‚ç™¾åˆ†æ¯”è®¡ç®—ä½¿ç”¨ã€‚
æ­¤å¤„ä½¿ç”¨äº†è‡ªç„¶å¸¸æ•°eçš„æŒ‡æ•°å‡½æ•°å»è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚
$$ \text{softmax}(x)_i = \frac{e^{xi}}{\sum_{j}e^{x_j}} $$
**Step5:æœ€ç»ˆçš„attentionå…¬å¼**
$$
attention(Q,K,V) = softmax(\frac{QK^T}{âˆšd_k})v
$$

### 3. æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°
```python
'''æ³¨æ„åŠ›è®¡ç®—å‡½æ•°'''
def attention(query, key, value, dropout=None):
    '''
    args:
    query: æŸ¥è¯¢å€¼çŸ©é˜µ
    key: é”®å€¼çŸ©é˜µ
    value: çœŸå€¼çŸ©é˜µ
    '''
    # è·å–é”®å‘é‡çš„ç»´åº¦ï¼Œé”®å‘é‡çš„ç»´åº¦å’Œå€¼å‘é‡çš„ç»´åº¦ç›¸åŒ
    d_k = query.size(-1) 
    # è®¡ç®—Qä¸Kçš„å†…ç§¯å¹¶é™¤ä»¥æ ¹å·dk
    # transposeâ€”â€”ç›¸å½“äºè½¬ç½®
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    # Softmax
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
        # é‡‡æ ·
     # æ ¹æ®è®¡ç®—ç»“æœå¯¹valueè¿›è¡ŒåŠ æƒæ±‚å’Œ
    return torch.matmul(p_attn, value), p_attn

```
å…¶ä¸­ï¼Œdropoutæ˜¯åšäº†p_attnçš„çŸ©é˜µæƒé‡æš‚æ—¶å½’é›¶ï¼Œé˜²æ­¢â€è¿‡æ‹Ÿåˆâ€œï¼ˆOverfittingï¼‰å’Œä¿è¯å¥½çš„é²æ£’æ€§

returnï¼šç»“æœã€çŸ©é˜µ
ç»“æœä¸­çš„â€œç›®æ ‡è¯â€ä¸ä»…ä»…å­˜åœ¨valueç»“æœï¼Œè¿˜å­˜åœ¨ä¸€ä¸ªåŒ…å«äº†ä¸Šä¸‹æ–‡å­¦ä¹ çš„ï¼ˆçŸ©é˜µä¿¡æ¯ï¼‰ã€‚

### 4. è‡ªæ³¨æ„åŠ›
Self-Attention

æ³¨æ„åŠ›å®Œå…¨å‘ç”Ÿåœ¨è¾“å…¥çš„åºåˆ—å†…éƒ¨ï¼Œä¸€ä¸ªè¯åœ¨è‡ªå·±å¥å­å†…éƒ¨çš„æ³¨æ„åŠ›ã€‚è€Œä¸æ˜¯è¾“å…¥ä¸€ä¸ªQï¼Œåœ¨å¦å¤–æä¾›çš„KVä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚

Self-Attentionä¼šå®šä¹‰æ¯ä¸ªè¯çš„æƒé‡çŸ©é˜µï¼ˆWeight Matrixï¼‰ï¼›
$$
W^Q,W^K,W^V
$$
è¿™ä¸‰è€…ä¼šå„è‡ªè´Ÿè´£æŠŠè¯å‘é‡xè½¬åŒ–ä¸ºé€‚é…è‡ªå·±å¯¹åº”Queryï¼ŒKeyï¼ŒValueçš„å‘é‡ã€‚ç”±ä¸€ä¸ªè¾“å…¥ï¼Œå¾—å‡ºQKVã€‚

ä½œç”¨ï¼š
- çœŸæ­£çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šå¥å­ä¸­ä»»æ„ä¸¤ä¸ªè¯å¯ä»¥ç›´æ¥è®¡ç®—å…³ç³»ï¼Œä¸å—è·ç¦»é™åˆ¶ï¼Œè§£å†³äº†LSTMçš„é—®é¢˜ã€‚
- é«˜å¹¶è¡Œè¿ç®—ï¼šèƒ½å¤ŸåŒæ—¶è®©æ‰€æœ‰è¯è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œè€Œä¸æ˜¯ä»…è®¡ç®—ä¸€ä¸ªè¯
- TransformeråŸºçŸ³ï¼šç”±å¤šä¸ªSelf-Attention+å‰åé¦ˆç½‘ç»œç»„æˆ

æ„ä¹‰ï¼šAttentionæ˜¯åŸºäºLSTMä¸²è¡Œæ¶æ„ä¸ºæ ¸å¿ƒ+æ³¨æ„åŠ›æ¨¡å—ä¸ºè¾…åŠ©çš„ä¸Šä¸‹æ–‡ç†è§£ï¼›Self-Attentionæ˜¯æ³¨æ„åŠ›æ¨¡å—ä¸ºæ ¸å¿ƒæ„å»ºçš„ä¸Šä¸‹æ–‡ç†è§£ã€‚

>è®¡ç®—æœºå¯¹ä¸€ä¸ªå¥å­çš„é˜…è¯»èƒ½åŠ›
>**Word2Vec**
> é™æ€ç¿»è¯‘ï¼Œä¸€ä¸ªè¯ä¸€ä¸ªå‘é‡å€¼ï¼Œæ— æ³•ç†è§£ä¸Šä¸‹æ–‡è§£å†³å¤šä¹‰è¯é—®é¢˜
>**ELMo**
>å®ç°å¥å­å†…éƒ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å»ç”Ÿæˆå‘é‡ï¼Œ**åŠ¨æ€å‘é‡**ã€‚å±€é™æ€§æ˜¯è¶…é•¿LSTMç†è§£
>**Attention**
>Encoder-Decoder æ¶æ„
>ç¿»è¯‘å·¥ä½œï¼Œç›®æ ‡è¯å»èšç„¦æºå¤´çš„é‚£ä¸€å—å†…å®¹å®ç°ç¿»è¯‘
>**Self-Attention**
>ä¸ä½¿ç”¨LSTMæ¶æ„ï¼Œè¯¥ç”¨å¹¶è¡Œæ¶æ„ã€‚

### 5. æ©ç è‡ªæ³¨æ„åŠ›
Mask Self-Attention

**æ©ç ä½œç”¨**ï¼šé®è”½æ‰ä¸€äº›ç‰¹å®šä½ç½®çš„tokenï¼Œæ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä¼šå¿½ç•¥æ‰é®è”½çš„tokenã€‚æ ¸å¿ƒåœ¨äºä¸è®©æ¨¡å‹çœ‹åˆ°æœªæ¥ä¿¡æ¯ã€‚åªèƒ½é€šè¿‡ä¹‹å‰çš„tokenå»é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚

**æ©ç æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ï¼š**
è®¡ç®—å®ŒQK^T åï¼Œé€å…¥Softmaxä¹‹å‰ä¼šå¾—åˆ°â€œåŸå§‹æ³¨æ„åŠ›åˆ†æ•°â€çŸ©é˜µ`Scoresã€‚`

```
æˆ‘ çˆ± ä½  (Key) 
æˆ‘ [10, 8, 5] 
çˆ± [ 9, 12, 11] 
ä½  [ 4, 7, 9] 
(Query)
```

ç”Ÿæˆä¸€ä¸ªæ©ç çŸ©é˜µ`mask`
```
æˆ‘ çˆ± ä½  
æˆ‘ [ 0, -âˆ, -âˆ] 
çˆ± [ 0, 0, -âˆ] 
ä½  [ 0, 0, 0]
```

å®ç°æ–¹å¼ï¼šé€å…¥Softmaxä¹‹å‰è¿›è¡Œåˆ†æ•°çŸ©ä¸æ©ç çŸ©é˜µç›¸åŠ ã€‚
`Scores_masked=Scores+mask`

æœ€åé€šè¿‡Softmaxï¼ŒæŠŠ`-âˆ`å¤„ç†ä¸º0åˆ™å¾—åˆ°äº†ï¼š
```
æˆ‘ çˆ± ä½  æˆ‘ [1.0, 0.0, 0.0] // â€œæˆ‘â€åªèƒ½100%å…³æ³¨è‡ªå·± 
çˆ± [0.2, 0.8, 0.0] // â€œçˆ±â€çš„æ³¨æ„åŠ›åˆ†é…ç»™äº†â€œæˆ‘â€å’Œâ€œçˆ±â€ 
ä½  [0.1, 0.4, 0.5] // â€œä½ â€çš„æ³¨æ„åŠ›åˆ†é…ç»™äº†æ‰€æœ‰äºº
```

**è¯¥æ–¹æ³•åº”ç”¨äºDecoderéƒ¨åˆ†ï¼Œå…¶å…³é”®åœ¨äºå®ç°å¹¶è¡Œè®¡ç®—åºåˆ—é¢„æµ‹ä»»åŠ¡**

### 6. å¤šå¤´æ³¨æ„åŠ›
Multi-Head Attention

ä¹‹å‰çš„çš„è‡ªæ³¨æ„åŠ›ç¼©æ”¾ç‚¹ç§¯ã€QK^Tã€Maskã€Scoreçš„å®Œæ•´æµç¨‹å±äºå•å¤´æ³¨æ„åŠ›ï¼Œè€Œå¤šå¤´æ³¨æ„åŠ›å¯ä»¥ç†è§£ä¸ºâ€œå›¢é˜Ÿåä½œâ€æœºåˆ¶ã€‚

è‡ªæ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†å¹¶è¡Œé—®é¢˜ï¼Œä½†æ˜¯å…¶ä¸­è¿˜å­˜åœ¨å•å¤´åªèƒ½æ‹Ÿåˆ**ä¸€ç§ç›¸å…³å…³ç³»**ã€‚ï¼ˆå¦‚è¯­æ³•ä¸€ç±»ï¼Œæ— æ³•æ‹Ÿåˆå…¶ä»–å…³ç³»ï¼‰è¿™å—ç†è§£ä¸ºè¿›è¡Œ**è‡ªæ³¨æ„åŠ›ç‚¹ç§¯**çš„æ—¶å€™ï¼Œåªèƒ½**å…è®¸ä¸€ç§å…³ç³»çš„è‡ªæ³¨æ„åŠ›ç‚¹ç§¯**ã€‚

![](inbox/Pasted%20image%2020250828104226.png)
>å›¾æºï¼šè®ºæ–‡ï¼šAttention Is All Need
>å›¾ä¸­ä¸¤å±‚ä¸ºä¸¤ä¸ªæ³¨æ„åŠ›å¤´å¯¹åŒä¸€æ®µè¯­å¥åºåˆ—è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—çš„ç»“æœã€‚å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºä¸åŒçš„æ³¨æ„åŠ›å¤´ï¼Œèƒ½å¤Ÿæ‹Ÿåˆä¸åŒå±‚æ¬¡çš„ç›¸å…³ä¿¡æ¯ã€‚

â€œå‡è®¾æ¨¡å‹ç»´åº¦çš„`d_model=512`ï¼Œå¤´æ•°ä¸º`h=8`ï¼Œå‡†å¤‡8ç»„ç‹¬ç«‹çš„W^qW^kW^vï¼Œè¾“å…¥è¯å‘é‡Xåˆ†åˆ«ä¹˜ä»¥è¿™8ç»„ä¸¾è¯ï¼Œå¾—åˆ°çŸ©é˜µåï¼Œæ¯ä¸€ç»„é™ç»´ä¸º64ã€‚

**æ¨¡å‹ç»´åº¦**ï¼š`d_model`ç”±äººå·¥ç¡®å®šï¼Œæ ¹æ®æ€§èƒ½ä¸æˆæœ¬å»è¡¡é‡ã€‚
åœ¨Input Embeddingä¸­ï¼Œä¸€ä¸ªå•è¯ï¼ˆTokenï¼‰è¾“å…¥åä¼šè½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡ï¼Œå…¶é•¿åº¦ï¼ˆç»´åº¦ï¼‰å³ä¸º`d_model`ã€‚

ä»£ç å¦‚ä¸‹ï¼š
```python
import torch.nn as nn
import torch

'''
è¯¥é¡¹ç›®å…ˆé‡‡ç”¨äº†PyTorchçš„nn.Moduleå»æ”¹é€ ã€‚
å‡†å¤‡å·¥ä½œï¼š__init__å‡½æ•°ï¼ˆæ„é€ å‡½æ•°ï¼‰æ„å»ºæ–¹æ³•ï¼Œåªè¿è¡Œä¸€æ¬¡
å·¥ä½œé˜¶æ®µï¼šforwardå‡½æ•° å®šä¹‰æ•°æ®æµè¿‡è¿è¡Œï¼Œå¤šæ¬¡è°ƒç”¨

super().__init__()è°ƒç”¨çˆ¶ç±»nn.Moduleçš„æ–¹æ³•
åç»­ä¼šè°ƒç”¨å…¶ä¸­çš„æ•°æ®ç±»å‹ä½œä¸ºä½¿ç”¨

args.dim æ¨¡å‹çš„æ€»ç»´åº¦
args.n_heads æ¨¡å‹çš„å¤´æ•°
self.head_dim æ¯ä¸ªå¤´çš„ç»´åº¦
args.n_embd ï¼Ÿ

nn.Linear()PyTorchåˆ›å»ºå…¨è¿æ¥å±‚å‘½ä»¤ï¼Œå³æƒé‡çŸ©Wqkv,Woç”¨äºå®¹çº³æ‰€æœ‰å¤´çš„ç»“æœ
nn.Linear(input_tensor,output_tensor,bias=True)è¾“å…¥å¼ é‡ã€è¾“å‡ºå¼ é‡ã€åç½®
'''

'''å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æ¨¡å—'''
class MultiHeadAttention(nn.Module): 

    def __init__(self, args: ModelArgs, is_causal=False):
        # æ„é€ å‡½æ•°
        # args: é…ç½®å¯¹è±¡
        super().__init__()
        # assert å®‰å…¨æ£€æŸ¥æ•´é™¤ã€‚éšè—å±‚ç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€ï¼Œå› ä¸ºåé¢æˆ‘ä»¬ä¼šå°†è¾“å…¥æ‹†æˆå¤´æ•°ä¸ªçŸ©é˜µ
        assert args.dim % args.n_heads == 0
        # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œç­‰äºæ¨¡å‹ç»´åº¦é™¤ä»¥å¤´çš„æ€»æ•°ã€‚
        self.head_dim = args.dim // args.n_heads
        self.n_heads = args.n_heads

        # Wq, Wk, Wv å‚æ•°çŸ©é˜µï¼Œæ¯ä¸ªå‚æ•°çŸ©é˜µä¸º n_embd x dim
        # è¿™é‡Œé€šè¿‡ä¸‰ä¸ªç»„åˆçŸ©é˜µæ¥ä»£æ›¿äº†nä¸ªå‚æ•°çŸ©é˜µçš„ç»„åˆï¼Œå…¶é€»è¾‘åœ¨äºçŸ©é˜µå†…ç§¯å†æ‹¼æ¥å…¶å®ç­‰åŒäºæ‹¼æ¥çŸ©é˜µå†å†…ç§¯ï¼Œ
        # ä¸ç†è§£çš„è¯»è€…å¯ä»¥è‡ªè¡Œæ¨¡æ‹Ÿä¸€ä¸‹ï¼Œæ¯ä¸€ä¸ªçº¿æ€§å±‚å…¶å®ç›¸å½“äºnä¸ªå‚æ•°çŸ©é˜µçš„æ‹¼æ¥
        self.wq = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        # è¾“å‡ºæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º dim x dimï¼ˆhead_dim = dim / n_headsï¼‰
        self.wo = nn.Linear(self.n_heads * self.head_dim, args.dim, bias=False)
        # æ³¨æ„åŠ›çš„ dropoutï¼Œç”¨äºæ³¨æ„åŠ›æƒé‡
        self.attn_dropout = nn.Dropout(args.dropout)
        # æ®‹å·®è¿æ¥çš„ dropoutï¼Œç”¨äºæœ€ç»ˆè¾“å‡ºç”¨çš„
        self.resid_dropout = nn.Dropout(args.dropout)
        self.is_causal = is_causal

        # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºé®è”½æœªæ¥ä¿¡æ¯
        # æ³¨æ„ï¼Œå› ä¸ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ŒMask çŸ©é˜µæ¯”ä¹‹å‰æˆ‘ä»¬å®šä¹‰çš„å¤šä¸€ä¸ªç»´åº¦ã€‚åŸå› ï¼šåˆ©ç”¨å¹¿æ’­æœºåˆ¶ã€‚
        if is_causal:
            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float("-inf"))
            mask = torch.triu(mask, diagonal=1)
            # æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒº
            self.register_buffer("mask", mask)

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):

        # è·å–æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œ[batch_size, seq_len, dim]
        bsz, seqlen, _ = q.shape

        # è®¡ç®—æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰,è¾“å…¥é€šè¿‡å‚æ•°çŸ©é˜µå±‚ï¼Œç»´åº¦ä¸º (B, T, n_embed) x (n_embed, dim) -> (B, T, dim)
        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)

        # å°† Qã€Kã€V æ‹†åˆ†æˆå¤šå¤´ï¼Œç»´åº¦ä¸º (B, T, n_head, dim // n_head)ï¼Œç„¶åäº¤æ¢ç»´åº¦ï¼Œå˜æˆ (B, n_head, T, dim // n_head)
        # å› ä¸ºåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æˆ‘ä»¬æ˜¯å–äº†åä¸¤ä¸ªç»´åº¦å‚ä¸è®¡ç®—
        # ä¸ºä»€ä¹ˆè¦å…ˆæŒ‰B*T*n_head*C//n_headå±•å¼€å†äº’æ¢1ã€2ç»´åº¦è€Œä¸æ˜¯ç›´æ¥æŒ‰æ³¨æ„åŠ›è¾“å…¥å±•å¼€ï¼Œæ˜¯å› ä¸ºviewçš„å±•å¼€æ–¹å¼æ˜¯ç›´æ¥æŠŠè¾“å…¥å…¨éƒ¨æ’å¼€ï¼Œ
        # ç„¶åæŒ‰è¦æ±‚æ„é€ ï¼Œå¯ä»¥å‘ç°åªæœ‰ä¸Šè¿°æ“ä½œèƒ½å¤Ÿå®ç°æˆ‘ä»¬å°†æ¯ä¸ªå¤´å¯¹åº”éƒ¨åˆ†å–å‡ºæ¥çš„ç›®æ ‡
        '''
        xq.view()ç±»ä¼¼äºnumpyçš„.reshape(),[bsz,seqlen,512]-->[bsz,sqlen,8,64]
        xq.transpose(1,2)äº¤æ¢ç»´åº¦ï¼ŒæŠŠç¬¬1å’Œç¬¬2ç»´äº¤æ¢ï¼Œ[bsz,sqlen,8,64]-->[bsz,8,sqlen,64]
        æ­¤å¤„è½¬æ¢æ˜¯å› ä¸ºåç»­çš„matmulå¤„ç†å¼ é‡ä½¿ç”¨ï¼ŒBä¸ºæ‰¹æ¬¡ã€nhä¸ºå¤´æ•°ã€Tä¸ºå¥å­é•¿åº¦ã€hså¤´çš„ç»´åº¦ï¼‰
        â€œæˆ‘ä»¬æœ‰ `bsz` æ‰¹å¥å­ï¼Œæ¯å¥è¯æˆ‘ä»¬éƒ½æ´¾äº† `8` ä¸ªä¸“å®¶ï¼ˆå¤´ï¼‰å»åˆ†æï¼Œæ¯ä¸ªä¸“å®¶éƒ½æŠŠæ¯ä¸ªè¯ `T` è½¬æ¢æˆäº†ä¸€ä¸ª `64` ç»´çš„å‘é‡ã€‚â€
        '''
        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)
        xq = xq.transpose(1, 2)
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)
        # æ³¨æ„åŠ›è®¡ç®—
        # è®¡ç®— QK^T / sqrt(d_k)ï¼Œç»´åº¦ä¸º (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        '''
        transposeï¼ˆ2ï¼Œ3ï¼‰æ˜¯å¯¹æ¯ä¸ªå¤´çš„[seqlen,64]è¿›è¡ŒçŸ©é˜µè½¬ç½®ï¼Œå˜æˆ[64ï¼Œseplen]
        è€Œè®¡ç®—æ˜¯é’ˆå¯¹æ•´ä¸ª[bszï¼Œ8,...]å±•å¼€çš„ï¼Œä¸€æ¬¡æ€§è®¡ç®—äº†8ä¸ªå¤´çš„åˆ†æ•°
        torch.matmulï¼ˆaï¼Œbï¼‰ï¼Œa=[A,B,C,D],è®¡ç®—ä¸­Aï¼ŒBå½’å±æ‰¹æ¬¡ï¼Œè€ŒCï¼ŒDæ‰æ˜¯è®¡ç®—çš„çŸ©é˜µå†…å®¹
        è®¡ç®—æœ€åè¾“å‡ºçš„scoreså°±ä¸º[Tï¼ŒT]
        /math.sqrt(self.head_dim)å³ä¸ºé™æ¸©å¤„ç†ï¼ˆå³å‰æ–‡æåˆ°çš„ç¼©æ”¾8ï¼‰,é˜²æ­¢Softmaxä¹‹åï¼Œä¼ å›å­¦ä¹ å¯¼è‡´æ¢¯åº¦è¿‡å°æ— æ³•è¿›ä¸€æ­¥ä¼˜åŒ–
        '''
        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)
        # æ©ç è‡ªæ³¨æ„åŠ›å¿…é¡»æœ‰æ³¨æ„åŠ›æ©ç 
        if self.is_causal:
            assert hasattr(self, 'mask')
            # è¿™é‡Œæˆªå–åˆ°åºåˆ—é•¿åº¦ï¼Œå› ä¸ºæœ‰äº›åºåˆ—å¯èƒ½æ¯” max_seq_len çŸ­
            scores = scores + self.mask[:, :, :seqlen, :seqlen]
        # è®¡ç®— softmaxï¼Œç»´åº¦ä¸º (B, nh, T, T)
        '''
        F.softmax()çš„F.å±äºè®¡ç®—æ–¹å¼å¼•ç”¨ï¼Œä»Fä¸­è°ƒå‡ºsofmax()ï¼ŒFå½’å±äºtorché‡Œçš„ï¼Œè¿™é‡ŒåŒºåˆ†nn.Moduleï¼Œnn.Moduleå½’å±äºé›¶ä»¶ï¼Œè°ƒç”¨éœ€è¦å¯¹å†…éƒ¨å‚æ•°è¿›è¡Œå®šä¹‰ï¼Œè€ŒFå±äºç›´æ¥çš„functionå·¥å…·ï¼ŒåŸæœ¬ä¸ºimport torch.nn.functional as Fï¼Œè€Œåœ¨torchä¸­å…¶æœ¬è¢«as Fäº†ï¼Œä¸éœ€è¦è¿›è¡Œå¼•å…¥å®šä¹‰ã€‚
        scores.float()å°†å¼ é‡æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ›´åŠ ç²¾ç¡®ã€èŒƒå›´æ›´å¤§çš„float32ï¼Œè€ŒåŸæœ‰ä¸ºfloat16ä¼šç¼ºä¹ç²¾åº¦é¢ï¼Œå¹¶ä¸”e^xåä¼šå‡ºç°æ— ç©·å¤§ç»“æœinfã€‚æ­¤å¤„æ˜¯è½¬åŒ–float32åè¿›è¡Œsoftmaxè®¡ç®—ã€‚
        .type_as(xq)è½¬æ¢ä¸ºåŸæœ‰xqçš„æ•°æ®ç±»å‹float16.
        ä»¥ä¸Šæ³¨é‡Šçš„float16ä»…ä¸ºå‡è®¾ï¼Œfloat32çš„æ ¹æœ¬ç›®çš„æ˜¯æ›´åŠ ç²¾ç¡®ä¸è®¡ç®—ç¨³å®šã€‚
        '''
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        # åš Dropout
        '''
        éšæœºé®ä½äº†ä¸€éƒ¨åˆ†çš„scoresï¼ˆvq*vkï¼‰
        æœ€åç”±scores*wvå»è·å–å†…å®¹åŒ¹é…ä¸Š
        '''
        scores = self.attn_dropout(scores)
        # V * Scoreï¼Œç»´åº¦ä¸º(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        output = torch.matmul(scores, xv)
        # æ¢å¤æ—¶é—´ç»´åº¦å¹¶åˆå¹¶å¤´ã€‚
        # å°†å¤šå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥, å…ˆäº¤æ¢ç»´åº¦ä¸º (B, T, n_head, dim // n_head)ï¼Œå†æ‹¼æ¥æˆ (B, T, n_head * dim // n_head)
        # contiguous å‡½æ•°ç”¨äºé‡æ–°å¼€è¾Ÿä¸€å—æ–°å†…å­˜å­˜å‚¨ï¼Œå› ä¸ºPytorchè®¾ç½®å…ˆtransposeå†viewä¼šæŠ¥é”™ï¼Œ
        # å› ä¸ºviewç›´æ¥åŸºäºåº•å±‚å­˜å‚¨å¾—åˆ°ï¼Œç„¶è€Œtransposeå¹¶ä¸ä¼šæ”¹å˜åº•å±‚å­˜å‚¨ï¼Œå› æ­¤éœ€è¦é¢å¤–å­˜å‚¨
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)

        # æœ€ç»ˆæŠ•å½±å›æ®‹å·®æµã€‚
        '''
        scores*wvçš„ç»“æœoutputæ˜¯[Bï¼ŒTï¼Œ512]ï¼Œé‡Œé¢å®¹çº³äº†8ä¸ªå¤´çš„ï¼ˆä¸“å®¶ï¼‰ç»“æœï¼Œæ— æ³•è¯†åˆ«å“ªä¸€ä¸ªçš„æƒé‡æ›´åŠ é‡è¦ï¼Œæ•…åœ¨æ­¤åŠ å…¥äº†woå»åˆ†é…ä¸åŒä¸“å®¶çš„æƒé‡ã€‚
        woæ˜¯[512,512,bias=False]ï¼Œå…¶è®¡ç®—ç»“æœæ˜¯[Bï¼ŒTï¼Œ512]
        '''
        output = self.wo(output)
        output = self.resid_dropout(output)
        return output

```
è‡ªé—®è‡ªç­”ï¼š

## äºŒã€Encoder-Decoder
---
### 1. Seq2Seqæ¨¡å‹

### 2. å‰é¦ˆç¥ç»ç½‘ç»œ

### 3. å±‚å½’ä¸€åŒ–

### 4. æ®‹å·®è¿æ¥

### 5. Encoder

### 6. Decoder

## ä¸‰ã€æ­å»ºä¸€ä¸ªTransformer
---

### 1. Embeddingå±‚

### 2. ä½ç½®ç¼–ç 

### 3. ä¸€ä¸ªå®Œæ•´çš„Transformer