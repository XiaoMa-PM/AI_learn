{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374db4a6",
   "metadata": {},
   "source": [
    "创建一个文件夹openai-agent\n",
    "`cd openai-agent`\n",
    "加入该文件夹/在该文件夹打开终端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcb31b",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "设置虚拟环境openai-agent\n",
    "python -m venv openai-agent\n",
    "\n",
    "# 如果是mac或者linux系统，激活虚拟环境：\n",
    "source openai-agent/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900aef79",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# 确保pip是最新版本\n",
    "python3 -m pip install --upgrade pip\n",
    "# 查看当前所有pip\n",
    "python3 -m pip list\n",
    "# 查看某个pip版本\n",
    "python3 -m pip show <package-name>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e8111",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "python3 -m pip install \"openai-agents[litellm]\"\n",
    "pip install python-dotenv # 安装dotenv库，管理环境变量\n",
    "# 配置环境变量\n",
    "# 目录下新增一个`.env`文件，内容如下：\n",
    "DEEPSEEK_API_KEY=\"sk-xxxx...\"\n",
    "# 配置完成后，重启终端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b7627",
   "metadata": {},
   "source": [
    "注册jupyter内核\n",
    "虚拟环境中激活环境\n",
    "```bash\n",
    "cd /Users/archroot/Documents/Obsidian/wow-agent/agent-learning\n",
    "source bin/activate\n",
    "```\n",
    "安装ipykernel包：\n",
    "```bash\n",
    "pip install ipykernel\n",
    "```\n",
    "设置虚拟环境为Jupyter内核\n",
    "```bash\n",
    "python -m ipykernel install --user --name=agent-learning --display-name=\"Python (agent-learning)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d12657",
   "metadata": {},
   "source": [
    "创建python文件，以下步骤也可以分两步在jupyter运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "base_url = \"https://api.deepseek.com/v1\"\n",
    "chat_model = \"deepseek/deepseek-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/qwen2.5:7b\", \n",
    "    messages=[{ \"content\": \"你有什么技能？\",\"role\": \"user\"}], \n",
    "    api_base=\"http://192.168.0.123:11434\"\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e814ad",
   "metadata": {},
   "source": [
    "以上的`LitellmModel`，它像一个“万能转换插头”，非常强大。但教程接下来介绍的这种方法，是直接利用`openai`官方库的客户端`AsyncOpenAI`来配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22640d79",
   "metadata": {},
   "source": [
    "其方法与`LitellmModel`的配置方法相同，只是将`openai`替换为`AsyncOpenAI`。其中的`AsyncOpenAI`客户端是异步的，而`openai`客户端是同步的。\n",
    "`openai-agents`框架的核心逻辑与具体是哪个模型解耦了。它既可以接受像`LitellmModel`这样的通用适配器，也可以接受一个配置好的、符合规范的客户端。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8709c26",
   "metadata": {},
   "source": [
    "以下代码为异步运行的示例，不建议直接在jupyter notebook中运行，因为异步代码需要在事件循环中运行。\n",
    "`result = Runner.run_sync(agent, \"给我讲个程序员相亲的笑话\")`这个方法是同步运行的，会阻塞当前线程，直到模型返回结果。\n",
    "可以通过`import asyncio`引入异步io库，然后使用`asyncio.run()`方法运行异步代码。\n",
    "```python\n",
    "# 定义异步函数，用于执行异步逻辑\n",
    "async def main():\n",
    "    # 使用异步运行方法（通常框架会提供如 Runner.run 的异步接口）\n",
    "    result = await Runner.run(agent, \"给我讲个程序员相亲的笑话\")\n",
    "    print(result.final_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled, set_default_openai_api, set_default_openai_client\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = \"glm-4-flash\"\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
    "    api_key=os.getenv('zhipu_key'),\n",
    ")\n",
    "set_default_openai_client(client=client, use_for_tracing=False) # 全局变量\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "agent = Agent(name=\"Assistant\", model=chat_model, instructions=\"You are a helpful assistant\")\n",
    "result = Runner.run_sync(agent, \"给我讲个程序员相亲的笑话\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7e47c",
   "metadata": {},
   "source": [
    "一种是通用的`LitellmModel`，另一种是针对OpenAI兼容模型的`set_default_openai_client`全局设置。\n",
    "下方示例，使用了引入了一个更高级、更灵活的模型管理概念：`ModelProvider`（模型提供者）。这是`openai-agents`框架中一个非常优雅的设计，彻底实现了代码的解耦。\n",
    "- Agent 类负责定义智能体的“身份”和“职责”（name, instructions）。\n",
    "- ModelProvider 类负责“生产”具体的模型实例（get_model）。\n",
    "- Runner 负责执行任务，并在执行时将两者“撮合”在一起。\n",
    "\n",
    "区别了之前的`set_default_openai_client(client=client, use_for_tracing=False) `全局变量模型设置\n",
    "- 为了应用于多Agent\n",
    "- 应用于A/B 测试\n",
    "\n",
    "设计上使用了`CUSTOM_MODEL_PROVIDER = CustomModelProvider()`动态提供模型\n",
    "使用`.py`运行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所有需要的类和函数\n",
    "# 从 agents 库导入核心组件，这是构建和运行智能体的基础\n",
    "from agents import (\n",
    "    Agent,                      # 用于定义一个智能体（角色、指令、模型等）\n",
    "    Model,                      # 模型类的基类，用于类型提示\n",
    "    ModelProvider,              # “模型提供者”的基类，我们要继承它来实现自定义\n",
    "    OpenAIChatCompletionsModel, # 一个具体的模型实现，用于包装任何兼容OpenAI API的聊天模型\n",
    "    RunConfig,                  # “运行配置”类，用于在运行时传递特定配置\n",
    "    Runner,                     # “运行器”，负责执行Agent的任务\n",
    "    set_tracing_disabled,       # 一个辅助函数，用于关闭详细的日志追踪\n",
    ")\n",
    "# 从 openai 库导入异步客户端，用于和模型API通信\n",
    "from openai import AsyncOpenAI\n",
    "# 导入 os 和 dotenv 用于加载环境变量（API密钥）\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "\n",
    "# --- 第一部分：环境和模型客户端的基础配置 ---\n",
    "\n",
    "# 加载 .env 文件中的环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 定义我们将要使用的智谱模型名称\n",
    "chat_model = \"glm-4-flash\"\n",
    "\n",
    "# 创建一个专门用于连接“智谱AI”的 AsyncOpenAI 客户端实例\n",
    "# 这就像是创建了一个专门拨打智谱API热线的“电话机”\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\",  # 智谱API的地址\n",
    "    api_key=os.getenv('zhipu_key'),                   # 从环境变量中读取智谱的API密钥\n",
    ")\n",
    "\n",
    "# chat_model = \"mistral-small-latest\"\n",
    "# client = AsyncOpenAI(\n",
    "#     base_url=\"https://api.mistral.ai/v1\",\n",
    "#     api_key=os.getenv('mistral_key'),\n",
    "# )\n",
    "\n",
    "# --- 第二部分：定义核心的“模型提供者” ---\n",
    "\n",
    "# 我们定义一个自己的类，继承自 ModelProvider\n",
    "# 它的作用就像一个“模型工厂”，专门生产我们想要的模型实例\n",
    "class CustomModelProvider(ModelProvider):\n",
    "    # 必须实现 get_model 方法，这是 ModelProvider 的“合同要求”\n",
    "    def get_model(self) -> Model:\n",
    "        # 这个方法被调用时，它会创建并返回一个配置好的模型实例\n",
    "        # 这里，它返回一个使用上面配置好的智谱客户端和模型名称的模型对象\n",
    "        return OpenAIChatCompletionsModel(model=chat_model, openai_client=client)\n",
    "\n",
    "# 创建这个“模型工厂”的一个全局实例，方便后面使用\n",
    "CUSTOM_MODEL_PROVIDER = CustomModelProvider()\n",
    "\n",
    "# --- 第三部分：创建 Agent 并配置运行 ---\n",
    "\n",
    "# 禁用框架的详细日志追踪，让输出更简洁\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "# 创建一个 Agent 实例\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    # 注意：这里我们仍然为 Agent 指定了一个“默认”模型。\n",
    "    # 但在接下来的运行中，这个默认模型将被“覆盖”。\n",
    "    model=OpenAIChatCompletionsModel(model=chat_model, openai_client=client),\n",
    "    instructions=\"You are a helpful assistant\"\n",
    ")\n",
    "\n",
    "# 定义主函数\n",
    "def main():\n",
    "    # 使用 Runner.run_sync 来同步执行 Agent 的任务\n",
    "    result = Runner.run_sync(\n",
    "        agent,  # 要运行的Agent\n",
    "        \"外观设计专利保护期多少年？\",  # 要Agent处理的任务（问题）\n",
    "        \n",
    "        # --- 这是最关键的一步 ---\n",
    "        # 我们创建了一个 RunConfig 对象，并把我们的“模型工厂”传了进去\n",
    "        # 这就像是给这次任务下达了一个“特殊指令”：不要用默认模型，用这个提供者给的模型！\n",
    "        run_config=RunConfig(model_provider=CUSTOM_MODEL_PROVIDER),\n",
    "    )\n",
    "    # 打印Agent执行任务后的最终输出\n",
    "    print(result.final_output)\n",
    "\n",
    "# Python脚本的入口点\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db29aeb",
   "metadata": {},
   "source": [
    "目前，以这种方式配置 智谱、deepseek、和mistral都不支持异步运行，需要用Runner.run_sync 去运行。--2025/10/18\n",
    "\n",
    "设置模型的配置：温度值、语言等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34908778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个名为 \"English agent\" 的 Agent 实例\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    # 设定Agent的系统指令：你只说英语\n",
    "    instructions=\"You only speak English\",\n",
    "    \n",
    "    # --- 关键部分 ---\n",
    "    # 创建一个ModelSettings实例，并将其传给 model_settings 参数\n",
    "    model_settings=ModelSettings(temperature=0.1,max_tokens=10,),#max_token限制输出的token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
